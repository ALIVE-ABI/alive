{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8923be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import radians, ndarray, sin, cos, degrees, arctan2, arcsin, tan, arccos\n",
    "from google.cloud import storage\n",
    "import datetime\n",
    "from scipy.interpolate import CubicSpline\n",
    "from multiprocessing import Pool\n",
    "from pyproj import CRS\n",
    "import rioxarray as rxr\n",
    "import h5netcdf\n",
    "# import pickle5 as pickle\n",
    "import joblib\n",
    "import fsspec\n",
    "from datetime import date, datetime, timedelta\n",
    "import time\n",
    "import dask.array\n",
    "import cftime\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numexpr as ne\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure arraylake/icechunk\n",
    "import arraylake as al\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import icechunk\n",
    "from icechunk.xarray import to_icechunk\n",
    "\n",
    "import coiled\n",
    "import dask\n",
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "import rasterio\n",
    "from rasterio.crs import CRS\n",
    "from rasterio.transform import from_origin\n",
    "import s3fs\n",
    "from pyproj import CRS, Transformer\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import itertools\n",
    "\n",
    "today = date.today()\n",
    "doy = date.today().timetuple().tm_yday\n",
    "print(today)\n",
    "\n",
    "# python 3.11.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad6e084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure arraylake\n",
    "# from arraylake import Client\n",
    "# aclient = al.Client()\n",
    "# aclient.login()\n",
    "# repo = aclient.get_repo('wisc-alive/ALIVE_v3-icechunk-lightweight-model-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8928d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coiled.create_software_environment(\n",
    "#     name=\"alive-gpp-env2\",\n",
    "#     conda=\"/Users/Sophie/Desktop/environment_final.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b64a275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained models\n",
    "DSRmodel = joblib.load(\"/Users/Sophie/Desktop/ALIVE_ET_V3/hgbr_dsr_model_20250702.pkl\")\n",
    "# GPPmodel = joblib.load(\"/Users/Sophie/Desktop/alive_version2/GPP_GBR_2025-05-28.pkl\")\n",
    "LSTmodel = joblib.load(\"/Users/Sophie/Desktop/ALIVE_ET_V3/hgbr_lst_model_20250702.pkl\")\n",
    "\n",
    "mean_model = joblib.load(\"/Users/Sophie/Desktop/ALIVE_ET_V3/test models/trained_mean_models.pkl\")\n",
    "quantile_model = joblib.load(\"/Users/Sophie/Desktop/ALIVE_ET_V3/test models/trained_quantile_models.pkl\")\n",
    "uncertainty_model = joblib.load(\"/Users/Sophie/Desktop/ALIVE_ET_V3/test models/trained_random_uncertainty_models.pkl\")\n",
    "\n",
    "# targets = ['ETcea_NEON', 'Tcea_NEON', 'Ecea_NEON', 'NEE_PI', 'GPP_np', 'RECO_np']\n",
    "GPPmodel = mean_model['GPP_np']\n",
    "ETmodel = mean_model['ETcea_NEON']\n",
    "Tmodel = mean_model['Tcea_NEON']\n",
    "Emodel = mean_model['Ecea_NEON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42e05cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABIangle2LonLat(x, y, H, req, rpol, lon_0_deg):\n",
    "    print('Finding latitude and lingutude of array point...')\n",
    "    \n",
    "    '''This function finds the latitude and longitude (degrees) of point P\n",
    "    given x and y, the ABI elevation and scanning angle (radians)'''\n",
    "\n",
    "    # Intermediate calculations\n",
    "    a = np.sin(x)**2 + ( np.cos(x)**2 * ( np.cos(y)**2 + ( req**2 / rpol**2 ) * np.sin(y)**2 ) )\n",
    "    b = -2 * H * np.cos(x) * np.cos(y)\n",
    "    c = H**2 - req**2\n",
    "\n",
    "    rs = ( -b - np.sqrt( b**2 - 4*a*c ) ) / ( 2 * a ) # distance from satellite point (S) to P\n",
    "\n",
    "    Sx = rs * np.cos(x) * np.cos(y)\n",
    "    Sy = -rs * np.sin(x)\n",
    "    Sz = rs * np.cos(x) * np.sin(y)\n",
    "\n",
    "    # Calculate lat and lon\n",
    "    lat = np.arctan( ( req**2 / rpol**2 ) * ( Sz / np.sqrt( ( H - Sx )**2 + Sy**2 ) ) )\n",
    "    lat = np.degrees(lat) #*\n",
    "    lon = lon_0_deg - np.degrees( np.arctan( Sy / ( H - Sx )) )\n",
    "    print('Latitude:', lat, 'Longitude:', lon)\n",
    "    return (lon,lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de67a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcc_to_latlon(x, y):\n",
    "    print('Finding latitude and lingutude of array point...')\n",
    "\n",
    "    \"\"\"\n",
    "    Convert LCC-projected x and y coordinates to lat/lon grids.\n",
    "    \"\"\"\n",
    "    # Define CRS\n",
    "    lcc_proj4=\"+proj=lcc +lat_0=25 +lat_1=25 +lon_0=-95 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    "    lcc_crs = CRS.from_proj4(lcc_proj4)\n",
    "    wgs84_crs = CRS.from_epsg(4326)  # Lat/lon\n",
    "\n",
    "    # Create transformer\n",
    "    transformer = Transformer.from_crs(lcc_crs, wgs84_crs, always_xy=True)\n",
    "\n",
    "    # Create 2D meshgrid from x and y\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    # Transform to lon/lat\n",
    "    lon, lat = transformer.transform(xx, yy)\n",
    "    print('Latitude:', lat, 'Longitude:', lon)\n",
    "    return (lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37e00a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name,file_prefix):\n",
    "    storage_client = storage.Client.create_anonymous_client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix = file_prefix) # list of files from that bucket & prefix\n",
    "\n",
    "    blob_list = list(blobs)\n",
    "    count = len(blob_list)\n",
    "    return(count,blob_list,bucket) # number of files, list of files, bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed9ef789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(product, year, day, hour, bucket_name):\n",
    "    folder_path = 'ABI-L2-{product}/{year}/{day}/{hour}/'.format(product = product, year = year,\n",
    "                                                                 day = day, hour = hour)\n",
    "    numBlobs, productBlobs, bucket = list_blobs(bucket_name, folder_path)\n",
    "    image_list = []\n",
    "\n",
    "    for i in range(0, numBlobs):\n",
    "        theBlob = productBlobs[i]\n",
    "        location, blob_filename = os.path.split(theBlob.name)\n",
    "        url = \"https://storage.googleapis.com/gcp-public-data-goes-16/\" + theBlob.name\n",
    "        image_list.append(url)\n",
    "    print(product,' IMAGE LIST -----------------------------------')\n",
    "    print(image_list)\n",
    "    return(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a8e4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_array_to_match(data_array, crs, transform, match_ds):\n",
    "    # print('Performing reprojections...')\n",
    "    # Extract shape\n",
    "    ny, nx = data_array.shape\n",
    "\n",
    "    # Generate x and y coordinate arrays from the affine transform\n",
    "    x_coords = transform.c + transform.a * (np.arange(nx) + 0.5)\n",
    "    y_coords = transform.f + transform.e * (np.arange(ny) + 0.5)\n",
    "\n",
    "    # Create DataArray with proper dims and coords\n",
    "    da = xr.DataArray(\n",
    "        data_array.values[np.newaxis, :, :],\n",
    "        dims=[\"band\", \"y\", \"x\"],\n",
    "        coords={\"band\": [1], \"y\": y_coords, \"x\": x_coords},\n",
    "        name=\"data_band\"\n",
    "    ).rio.write_crs(crs).rio.write_transform(transform)\n",
    "    # print('Finished reprojections.')\n",
    "    # Reproject to match the target dataset\n",
    "    return da.rio.reproject_match(match_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8b9d6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSolar (ds, dt):\n",
    "    '''\n",
    "    x_grid = ds.x\n",
    "    y_grid = ds.y\n",
    "    print(x_grid,y_grid)\n",
    "\n",
    "    lon, lat = lcc_to_latlon(x_grid, y_grid)\n",
    "    lats = np.array(lat)\n",
    "    lons = np.array(lon)\n",
    "    '''\n",
    "    print('Performing solar calculations...')\n",
    "    x_grid = ds.x.values  # 1D\n",
    "    y_grid = ds.y.values  # 1D\n",
    "\n",
    "    lon, lat = lcc_to_latlon(x_grid, y_grid)  # Now 2D\n",
    "\n",
    "    # Ensure NaNs are replaced\n",
    "    latArray = np.nan_to_num(lat, nan=0)\n",
    "    lonArray = np.nan_to_num(lon, nan=0)\n",
    "\n",
    "    # from https://github.com/NASA-DEVELOP/dnppy/blob/master/dnppy/solar/README.md\n",
    "    if __name__ == \"__main__\":\n",
    "        my_datestamp   = dt                     # date stamp\n",
    "        my_fmt         = \"%Y-%m-%d %H:%M\"       # datestamp format\n",
    "        my_tz          = 0                      # timezone (GMT/UTC) offset\n",
    "        my_lat = latArray                       # lat (N positive)\n",
    "        my_lon = lonArray                       # lon (E positive)\n",
    "\n",
    "        sc  = solar(my_lat, my_lon, my_datestamp, my_tz, my_fmt)\n",
    "        SZA = sc.get_zenith()\n",
    "        SAA = sc.get_azimuth()\n",
    "    print('Finished solar calculations.')\n",
    "    return SZA, SAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5555554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class solar:\n",
    "    \"\"\"\n",
    "    from: https://github.com/NASA-DEVELOP/dnppy/blob/master/dnppy/solar/solar.py\n",
    "    author: 'Jwely'\n",
    "\n",
    "    Object class for handling solar calculations. Many equations are taken from the\n",
    "    excel sheet at this url : [http://www.esrl.noaa.gov/gmd/grad/solcalc/calcdetails.html]\n",
    "\n",
    "    It requires a physical location on the earth and a datetime object\n",
    "\n",
    "    :param lat:             decimal degrees latitude (float OR numpy array)\n",
    "    :param lon:             decimal degrees longitude (float OR numpy array)\n",
    "    :param time_zone:       float of time shift from GMT (such as \"-5\" for EST)\n",
    "    :param date_time_obj:   either a timestamp string following fmt or a datetime obj\n",
    "    :param fmt:             if date_time_obj is a string, fmt is required to interpret it\n",
    "    :param slope:           slope of land at lat,lon for solar energy calculations\n",
    "    :param aspect:          aspect of land at lat,lon for solar energy calculations\n",
    "\n",
    "    An instance of this class may have the following attributes:\n",
    "\n",
    "        =================== =========================================== ========\n",
    "        attribute           description                                 type\n",
    "        =================== =========================================== ========\n",
    "        lat                 latitude                                    (array)\n",
    "        lon                 longitude                                   (array)\n",
    "        tz                  time zone                                   (scalar)\n",
    "        rdt                 reference datetime object (date_time_obj)   (scalar)\n",
    "        slope               slope, derivative of DEM                    (array)\n",
    "        aspect              aspect (north is 0, south is 180)           (array)\n",
    "        ajd                 absolute julian day                         (scalar)\n",
    "        ajc                 absolute julian century                     (scalar)\n",
    "        geomean_long        geometric mean longitude of the sun         (scalar)\n",
    "        geomean_anom        geometric mean longitude anomaly of the sun (scalar)\n",
    "        earth_eccent        eccentricity of earths orbit                (scalar)\n",
    "        sun_eq_of_center    the suns equation of center                 (scalar)\n",
    "        true_long           true longitude of the sun                   (scalar)\n",
    "        true_anom           true longitude anomaly of the sun           (scalar)\n",
    "        app_long            the suns apparent longitude                 (scalar)\n",
    "        oblique_mean_elip   earth oblique mean ellipse                  (scalar)\n",
    "        oblique_corr        correction to earths oblique elipse         (scalar)\n",
    "        right_ascension     suns right ascension angle                  (scalar)\n",
    "        declination         solar declination angle                     (scalar)\n",
    "        equation_of_time    equation of time (minutes)                  (scalar)\n",
    "        hour_angle_sunrise  the hour angle at sunrise                   (array)\n",
    "        solar_noon          LST of solar noon                           (array)\n",
    "        sunrise             LST of sunrise time                         (array)\n",
    "        sunset              LST of sunset time                          (array)\n",
    "        sunlight            LST fractional days of sunlight             (array)\n",
    "        true_solar          LST for true solar time                     (array)\n",
    "        hour_angle          total hour angle                            (array)\n",
    "        zenith              zenith angle                                (array)\n",
    "        elevation           elevation angle                             (array)\n",
    "        azimuth             azimuthal angle                             (array)\n",
    "        rad_vector          radiation vector (distance in AU)           (scalar)\n",
    "        earth_distance      earths distance to sun in meters            (scalar)\n",
    "        norm_irradiance     incident solar energy at earth distance     (scalar)\n",
    "        =================== =========================================== ========\n",
    "\n",
    "    Units used by this class unless otherwise labeled\n",
    "\n",
    "      - angle =     degrees\n",
    "      - distance =  meters\n",
    "      - energy =    watts or joules\n",
    "      - time =      mostly in datetime objects. labeled in most cases.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lat, lon, date_time_obj, time_zone = 0,\n",
    "                         fmt = False, slope = None, aspect = None):\n",
    "        \"\"\"\n",
    "        Initializes critical spatial and temporal information for solar object.\n",
    "        \"\"\"\n",
    "\n",
    "        # empty list of class attributes\n",
    "        self.ajc                = None          # abs julian century (defined on __init__)\n",
    "        self.ajd                = None          # abs julian day (defined on __init__)\n",
    "        self.app_long           = None\n",
    "        self.atmo_refraction    = None\n",
    "        self.azimuth            = None\n",
    "        self.declination        = None\n",
    "        self.earth_distance     = None\n",
    "        self.earth_eccent       = None\n",
    "        self.elevation          = None\n",
    "        self.elevation_noatmo   = None\n",
    "        self.equation_of_time   = None\n",
    "        self.frac_day           = None\n",
    "        self.geomean_anom       = None\n",
    "        self.geomean_long       = None\n",
    "        self.hour_angle         = None\n",
    "        self.hour_angle_sunrise = None\n",
    "        self.lat                = lat           # lattitude (E positive)- float\n",
    "        self.lat_r              = radians(lat)  # lattitude in radians\n",
    "        self.lon                = lon           # longitude (N positive)- float\n",
    "        self.lon_r              = radians(lon)  # longitude in radians\n",
    "        self.norm_irradiance    = None\n",
    "        self.oblique_corr       = None\n",
    "        self.oblique_mean_elip  = None\n",
    "        self.rad_vector         = None\n",
    "        self.rdt                = None          # reference datetime (defined on __init__)\n",
    "        self.right_ascension    = None\n",
    "        self.solar_noon         = None\n",
    "        self.solar_noon_time    = None\n",
    "        self.sun_eq_of_center   = None\n",
    "        self.sunlight           = None\n",
    "        self.sunlight_time      = None\n",
    "        self.sunrise            = None\n",
    "        self.sunrise_time       = None\n",
    "        self.sunset             = None\n",
    "        self.sunset_time        = None\n",
    "        self.true_anom          = None\n",
    "        self.true_long          = None\n",
    "        self.true_solar         = None\n",
    "        self.true_solar_time    = None\n",
    "        self.tz                 = None          # time zone (defined on __init__)\n",
    "        self.zenith             = None\n",
    "\n",
    "        # slope and aspect\n",
    "        self.slope = slope\n",
    "        self.aspect = aspect\n",
    "\n",
    "        # Constants as attributes\n",
    "        self.sun_surf_rad       = 63156942.6    # radiation at suns surface (W/m^2)\n",
    "        self.sun_radius         = 695800000.    # radius of the sun in meters\n",
    "        self.orbital_period     = 365.2563630   # num of days it takes earth to revolve\n",
    "        self.altitude           = -0.01448623   # altitude of center of solar disk\n",
    "\n",
    "\n",
    "        # sets up the object with some subfunctions\n",
    "        self._set_datetime(date_time_obj, fmt, GMT_hour_offset = time_zone)\n",
    "\n",
    "        # specify if attributes are scalar floats or numpy array floats\n",
    "        if isinstance(lat, ndarray) and isinstance(lon, ndarray):\n",
    "            self.is_numpy   = True\n",
    "        else:\n",
    "            self.is_numpy   = False\n",
    "\n",
    "        return\n",
    "\n",
    "    def _set_datetime(self, date_time_obj, fmt = False, GMT_hour_offset = 0):\n",
    "        \"\"\"\n",
    "        sets the critical time information including absolute julian day/century.\n",
    "        Accepts datetime objects or a datetime string with format\n",
    "\n",
    "        :param date_time_obj:   datetime object for time of solar calculations. Will also\n",
    "                                accept string input with matching value for \"fmt\" param\n",
    "        :param fmt:             if date_time_obj is input as a string, fmt allows it to be\n",
    "                                interpreted\n",
    "        :param GMT_hour_offset: Number of hours from GMT for timezone of calculation area.\n",
    "        \"\"\"\n",
    "\n",
    "        # if input is datetime_obj set it\n",
    "        if isinstance(date_time_obj, datetime):\n",
    "            self.rdt =      date_time_obj\n",
    "            self.rdt +=     timedelta(hours = -GMT_hour_offset)\n",
    "\n",
    "        elif isinstance(date_time_obj, str) and isinstance(fmt, str):\n",
    "            self.rdt =      datetime.strptime(date_time_obj,fmt)\n",
    "            self.rdt +=     timedelta(hours = -GMT_hour_offset)\n",
    "        else:\n",
    "            raise Exception(\"bad datetime!\")\n",
    "\n",
    "        self.tz = GMT_hour_offset\n",
    "\n",
    "        # uses the reference day of january 1st 2000\n",
    "        jan_1st_2000_jd   = 2451545\n",
    "        jan_1st_2000      = datetime(2000,1,1,12,0,0)\n",
    "\n",
    "        time_del = self.rdt - jan_1st_2000\n",
    "        self.ajd = float(jan_1st_2000_jd) + float(time_del.total_seconds())/86400\n",
    "        self.ajc = (self.ajd - 2451545)/36525.0\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_geomean_long(self):\n",
    "        \"\"\" :return geomean_long: geometric mean longitude of the sun\"\"\"\n",
    "\n",
    "        if not self.geomean_long is None:\n",
    "            return self.geomean_long\n",
    "\n",
    "        self.geomean_long = (280.46646 + self.ajc * (36000.76983 + self.ajc*0.0003032)) % 360\n",
    "        return self.geomean_long\n",
    "\n",
    "\n",
    "    def get_geomean_anom(self):\n",
    "        \"\"\"calculates the geometric mean anomoly of the sun\"\"\"\n",
    "\n",
    "        if not self.geomean_anom is None:\n",
    "            return self.geomean_anom\n",
    "\n",
    "        self.geomean_anom = (357.52911 + self.ajc * (35999.05029 - 0.0001537 * self.ajc))\n",
    "        return self.geomean_anom\n",
    "\n",
    "\n",
    "    def get_earth_eccent(self):\n",
    "        \"\"\" :return earth_eccent: precise eccentricity of earths orbit at referece datetime \"\"\"\n",
    "\n",
    "        if not self.earth_eccent is None:\n",
    "            return self.earth_eccent\n",
    "\n",
    "        self.earth_eccent = 0.016708634 - self.ajc * (4.2037e-5 + 1.267e-7 * self.ajc)\n",
    "\n",
    "        return self.earth_eccent\n",
    "\n",
    "\n",
    "    def get_sun_eq_of_center(self):\n",
    "        \"\"\" :return sun_eq_of_center: the suns equation of center\"\"\"\n",
    "\n",
    "        if not self.sun_eq_of_center is None:\n",
    "            return self.sun_eq_of_center\n",
    "\n",
    "        if self.geomean_anom is None:\n",
    "            self.get_geomean_anom()\n",
    "\n",
    "        ajc = self.ajc\n",
    "        gma = radians(self.geomean_anom)\n",
    "\n",
    "        self.sun_eq_of_center = sin(gma) * (1.914602 - ajc*(0.004817 + 0.000014 * ajc)) + \\\n",
    "                                sin(2*gma) * (0.019993 - 0.000101 * ajc) + \\\n",
    "                                sin(3*gma) * 0.000289\n",
    "\n",
    "        return self.sun_eq_of_center\n",
    "\n",
    "\n",
    "    def get_true_long(self):\n",
    "        \"\"\" :return true_long: the tru longitude of the sun\"\"\"\n",
    "\n",
    "        if not self.true_long is None:\n",
    "            return self.true_long\n",
    "\n",
    "        if self.geomean_long is None:\n",
    "            self.get_geomean_long()\n",
    "\n",
    "        if self.sun_eq_of_center is None:\n",
    "            self.get_sun_eq_of_center()\n",
    "\n",
    "        self.true_long = self.geomean_long + self.sun_eq_of_center\n",
    "        return self.true_long\n",
    "\n",
    "\n",
    "    def get_app_long(self):\n",
    "        \"\"\" :return app_long: calculates apparent longitude of the sun\"\"\"\n",
    "\n",
    "        if not self.app_long is None:\n",
    "            return self.app_long\n",
    "\n",
    "        if self.true_long is None:\n",
    "            self.get_true_long()\n",
    "\n",
    "        stl = self.true_long\n",
    "        ajc = self.ajc\n",
    "\n",
    "        self.app_long = stl - 0.00569 - 0.00478 * sin(radians(125.04 - 1934.136 * ajc))\n",
    "        return self.app_long\n",
    "\n",
    "\n",
    "    def get_oblique_mean_elip(self):\n",
    "        \"\"\" :return oblique_mean_elip: oblique mean elliptic of earth orbit \"\"\"\n",
    "\n",
    "        if not self.oblique_mean_elip is None:\n",
    "            return self.oblique_mean_elip\n",
    "\n",
    "        ajc = self.ajc\n",
    "\n",
    "        self.oblique_mean_elip = 23 + (26 + (21.448 - ajc * (46.815 + ajc * (0.00059 - ajc * 0.001813)))/60)/60\n",
    "        return self.oblique_mean_elip\n",
    "\n",
    "\n",
    "    def get_oblique_corr(self):\n",
    "        \"\"\" :return oblique_corr:  the oblique correction \"\"\"\n",
    "\n",
    "        if not self.oblique_corr is None:\n",
    "            return self.oblique_corr\n",
    "\n",
    "        if self.oblique_mean_elip is None:\n",
    "            self.get_oblique_mean_elip()\n",
    "\n",
    "        ome = self.oblique_mean_elip\n",
    "        ajc = self.ajc\n",
    "\n",
    "        self.oblique_corr = ome + 0.00256 * cos(radians(125.04 - 1934.136 * ajc))\n",
    "        return self.oblique_corr\n",
    "\n",
    "\n",
    "    def get_declination(self):\n",
    "        \"\"\" :return declination: solar declination angle at ref_datetime\"\"\"\n",
    "\n",
    "        if not self.declination is None:\n",
    "            return self.declination\n",
    "\n",
    "        if self.app_long is None:\n",
    "            self.get_app_long()\n",
    "\n",
    "        if self.oblique_corr is None:\n",
    "            self.get_oblique_corr()\n",
    "\n",
    "        sal = radians(self.app_long)\n",
    "        oc  = radians(self.oblique_corr)\n",
    "\n",
    "        self.declination = degrees(arcsin((sin(oc) * sin(sal))))\n",
    "        return self.declination\n",
    "\n",
    "\n",
    "    def get_equation_of_time(self):\n",
    "        \"\"\" :return equation_of_time: the equation of time in minutes \"\"\"\n",
    "\n",
    "        if not self.equation_of_time is None:\n",
    "            return self.equation_of_time\n",
    "\n",
    "        if self.oblique_corr is None:\n",
    "            self.get_oblique_corr()\n",
    "\n",
    "        if self.geomean_long is None:\n",
    "            self.get_geomean_long()\n",
    "\n",
    "        if self.geomean_anom is None:\n",
    "            self.get_geomean_anom()\n",
    "\n",
    "        if self.earth_eccent is None:\n",
    "            self.get_earth_eccent()\n",
    "\n",
    "        oc  = radians(self.oblique_corr)\n",
    "        gml = radians(self.geomean_long)\n",
    "        gma = radians(self.geomean_anom)\n",
    "        ec  = self.earth_eccent\n",
    "\n",
    "        vary = tan(oc/2)**2\n",
    "\n",
    "        self.equation_of_time = 4 * degrees(vary * sin(2*gml) - 2 * ec * sin(gma) +\n",
    "                                4 * ec * vary * sin(gma) * cos(2 * gml) -\n",
    "                                0.5 * vary * vary * sin(4 * gml) -\n",
    "                                1.25 * ec * ec * sin(2 * gma))\n",
    "\n",
    "        return self.equation_of_time\n",
    "\n",
    "\n",
    "    def get_solar_noon(self):\n",
    "        \"\"\" :return solar_noon: solar noon in (local sidereal time LST)\"\"\"\n",
    "\n",
    "        if not self.solar_noon is None:\n",
    "            return self.solar_noon\n",
    "\n",
    "        if self.equation_of_time is None:\n",
    "            self.get_equation_of_time()\n",
    "\n",
    "        lon = self.lon\n",
    "        eot = self.equation_of_time\n",
    "        tz  = self.tz\n",
    "\n",
    "        self.solar_noon = (720 - 4 * lon - eot + tz * 60)/1440\n",
    "\n",
    "        # format this as a time for display purposes (Hours:Minutes:Seconds)\n",
    "        if self.is_numpy:\n",
    "            self.solar_noon_time = timedelta(days = self.solar_noon.mean())\n",
    "        else:\n",
    "            self.solar_noon_time = timedelta(days = self.solar_noon)\n",
    "\n",
    "        return self.solar_noon\n",
    "\n",
    "\n",
    "    def get_true_solar(self):\n",
    "        \"\"\" :return true_solar: true solar time at ref_datetime\"\"\"\n",
    "\n",
    "        if not self.true_solar is None:\n",
    "            return self.true_solar\n",
    "\n",
    "        if self.equation_of_time is None:\n",
    "            self.get_equation_of_time()\n",
    "\n",
    "        lon = self.lon\n",
    "        eot = self.equation_of_time\n",
    "\n",
    "\n",
    "        # turn reference datetime into fractional days\n",
    "        frac_sec = (self.rdt - datetime(self.rdt.year, self.rdt.month, self.rdt.day)).total_seconds()\n",
    "        frac_hr  = frac_sec / (60 * 60) + self.tz\n",
    "        frac_day = frac_hr / 24\n",
    "\n",
    "        self.frac_day = frac_day\n",
    "\n",
    "        # now get true solar time\n",
    "        self.true_solar = (frac_day * 1440 + eot + 4 * lon - 60 * self.tz) % 1440\n",
    "\n",
    "        # format this as a time for display purposes (Hours:Minutes:Seconds)\n",
    "        if self.is_numpy:\n",
    "            self.true_solar_time = timedelta(days = self.true_solar.mean() / (60*24))\n",
    "        else:\n",
    "            self.true_solar_time = timedelta(days = self.true_solar / (60*24))\n",
    "\n",
    "        return self.true_solar\n",
    "\n",
    "\n",
    "    def get_hour_angle(self):\n",
    "        \"\"\" :return hour_angle: returns hour angle at ref_datetime\"\"\"\n",
    "\n",
    "        if not self.hour_angle is None:\n",
    "            return self.hour_angle\n",
    "\n",
    "        if self.true_solar is None:\n",
    "            self.get_true_solar()\n",
    "\n",
    "        ts = self.true_solar\n",
    "\n",
    "        # matrix hour_angle calculations\n",
    "        if self.is_numpy:\n",
    "            ha = ts\n",
    "            ha[ha <= 0] = ha[ha <= 0]/4 + 180\n",
    "            ha[ha >  0] = ha[ha >  0]/4 - 180\n",
    "            self.hour_angle = ha\n",
    "\n",
    "        # scalar hour_angle calculations\n",
    "        else:\n",
    "            if ts <= 0:\n",
    "                self.hour_angle = ts/4 + 180\n",
    "            else:\n",
    "                self.hour_angle = ts/4 - 180\n",
    "\n",
    "        return self.hour_angle\n",
    "\n",
    "\n",
    "    def get_zenith(self):\n",
    "        \"\"\" :return zenith: returns solar zenith angle at ref_datetime\"\"\"\n",
    "\n",
    "        if not self.zenith is None:\n",
    "            return self.zenith\n",
    "\n",
    "        if self.declination is None:\n",
    "            self.get_declination()\n",
    "\n",
    "        if self.hour_angle is None:\n",
    "            self.get_hour_angle()\n",
    "\n",
    "        d   = radians(self.declination)\n",
    "        ha  = radians(self.hour_angle)\n",
    "        lat = self.lat_r\n",
    "\n",
    "        self.zenith = degrees(arccos(sin(lat) * sin(d) + cos(lat) * cos(d) * cos(ha)))\n",
    "\n",
    "        return self.zenith\n",
    "\n",
    "\n",
    "    def get_azimuth(self):\n",
    "        \"\"\" :return azimuth: returns solar azimuth angle at ref_datetime\"\"\"\n",
    "\n",
    "        if not self.azimuth is None:\n",
    "            return self.azimuth\n",
    "\n",
    "        if self.declination is None:\n",
    "            self.get_declination()\n",
    "\n",
    "        if self.hour_angle is None:\n",
    "            self.get_hour_angle()\n",
    "\n",
    "        if self.zenith is None:\n",
    "            self.get_zenith()\n",
    "\n",
    "        lat = self.lat_r\n",
    "        d   = radians(self.declination)\n",
    "        ha  = radians(self.hour_angle)\n",
    "        z   = radians(self.zenith)\n",
    "\n",
    "        # matrix azimuth calculations\n",
    "        # these equations are hideous monsters, but im not sure how to improve them without\n",
    "        # adding computational complexity.\n",
    "        if self.is_numpy:\n",
    "\n",
    "            az = ha * 0\n",
    "\n",
    "            az[ha > 0] = (degrees(arccos(((sin(lat[ha > 0]) * cos(z[ha > 0])) - sin(d)) / (cos(lat[ha > 0]) * sin(z[ha > 0])))) + 180) % 360\n",
    "            az[ha <=0] = (540 - degrees(arccos(((sin(lat[ha <=0]) * cos(z[ha <=0])) -sin(d))/ (cos(lat[ha <=0]) * sin(z[ha <=0]))))) % 360\n",
    "\n",
    "            self.azimuth = az\n",
    "\n",
    "        else:\n",
    "            if ha > 0:\n",
    "                self.azimuth = (degrees(arccos(((sin(lat) * cos(z)) - sin(d)) / (cos(lat) * sin(z)))) + 180) % 360\n",
    "            else:\n",
    "                self.azimuth = (540 - degrees(arccos(((sin(lat) * cos(z)) -sin(d))/ (cos(lat) * sin(z))))) % 360\n",
    "\n",
    "        return self.azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "718227b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "srf_hrrr_vars = 'SFCR', 'FRICV', 'SHTFL', 'LHTFL', 'CAPE', 'CIN', 'ULWRF', \\\n",
    "                'USWRF', 'DLWRF', 'DSWRF', 'VBDSF', 'VDDSF', 'HPBL', 'GFLUX','TMP'\n",
    "twom_hrrr_vars = 'SPFH','RH','TMP'\n",
    "hrrr_vars = srf_hrrr_vars + twom_hrrr_vars\n",
    "\n",
    "srf_hrrr_paths = [f\"surface/{var}/surface\" for var in srf_hrrr_vars]\n",
    "twom_hrrr_paths = [f\"2m_above_ground/{var}/2m_above_ground\" for var in twom_hrrr_vars]\n",
    "hrrr_paths = srf_hrrr_paths + twom_hrrr_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "243858bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_srf_hrrr_vars = 'SFCR', 'FRICV', 'LHTFL', 'CAPE', 'ULWRF', \\\n",
    "                'USWRF', 'DSWRF', 'VDDSF', 'HPBL', 'GFLUX','TMP'\n",
    "ET_twom_hrrr_vars = 'SPFH','RH','TMP'\n",
    "ET_hrrr_vars = ET_srf_hrrr_vars + ET_twom_hrrr_vars\n",
    "\n",
    "ET_srf_hrrr_paths = [f\"surface/{var}/surface\" for var in ET_srf_hrrr_vars]\n",
    "ET_twom_hrrr_paths = [f\"2m_above_ground/{var}/2m_above_ground\" for var in ET_twom_hrrr_vars]\n",
    "ET_hrrr_paths = ET_srf_hrrr_paths + ET_twom_hrrr_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abc71bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'gcp-public-data-goes-16'\n",
    "# day starts at 1 not 0\n",
    "\n",
    "day_start = 200\n",
    "day_stop = 201\n",
    "\n",
    "hour_list = [10]\n",
    "# hour_list = [0,1,10,11,12,13,14,15,16,17,18,19,20,21,22,23]\n",
    "# hour_list = list(range(0,24))\n",
    "\n",
    "year = 2024\n",
    "year_str = str(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "659b10d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:  2024  Day:  200\n",
      "Hour 10\n",
      "BRFC  IMAGE LIST -----------------------------------\n",
      "['https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001001180_e20242001003553_c20242001006319.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001006180_e20242001008553_c20242001011432.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001011180_e20242001013553_c20242001016525.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001016181_e20242001018553_c20242001021461.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001021181_e20242001023553_c20242001026284.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001026181_e20242001028553_c20242001031488.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001031180_e20242001033553_c20242001036362.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001036180_e20242001038553_c20242001041377.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001041181_e20242001043554_c20242001047048.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001046181_e20242001048553_c20242001051514.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001051181_e20242001053553_c20242001056586.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-BRFC/2024/200/10/OR_ABI-L2-BRFC-M6_G16_s20242001056181_e20242001058554_c20242001101361.nc']\n",
      "MCMIPC  IMAGE LIST -----------------------------------\n",
      "['https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001001180_e20242001003553_c20242001004083.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001006180_e20242001008553_c20242001009081.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001011180_e20242001013566_c20242001014086.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001016181_e20242001018565_c20242001019068.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001021181_e20242001023553_c20242001024073.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001026181_e20242001028565_c20242001029076.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001031180_e20242001033567_c20242001034077.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001036180_e20242001038565_c20242001039072.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001041181_e20242001043554_c20242001044067.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001046181_e20242001048565_c20242001049071.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001051181_e20242001053553_c20242001054078.nc', 'https://storage.googleapis.com/gcp-public-data-goes-16/ABI-L2-MCMIPC/2024/200/10/OR_ABI-L2-MCMIPC-M6_G16_s20242001056181_e20242001058560_c20242001059077.nc']\n"
     ]
    }
   ],
   "source": [
    "# Create list of GOES images (BRF and CMI)\n",
    "\n",
    "bucket_name = 'gcp-public-data-goes-16' #update to goes-19 for current dates\n",
    "\n",
    "BRFimages_all = []\n",
    "CMIimages_all = []\n",
    "\n",
    "for day in range(day_start,day_stop,1):\n",
    "    day_str = str(day).zfill(3)\n",
    "    print('Year: ',year,' Day: ',day_str)\n",
    "\n",
    "    for hour in hour_list:\n",
    "        hour_str = str(hour).zfill(2)\n",
    "        print('Hour', hour)\n",
    "\n",
    "        brf = get_images('BRFC', year_str, day_str, hour_str, bucket_name)\n",
    "        cmi = get_images('MCMIPC', year_str, day_str, hour_str, bucket_name)\n",
    "\n",
    "        min_len = min(len(brf), len(cmi))\n",
    "        BRFimages_all.extend(brf[:min_len])\n",
    "        CMIimages_all.extend(cmi[:min_len])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c38ed3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(BRFimages_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35eacdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRF images: 1\n",
      "CMI images: 1\n",
      "DatetimeIndex(['2024-07-18 10:01:18'], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "# keep only the first file in each hour\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def filter_hourly_goes_files(file_list):\n",
    "    times = []\n",
    "    for f in file_list:\n",
    "        # Extract the \"sYYYYJJJHHMMSS\" pattern\n",
    "        match = re.search(r's(\\d{4})(\\d{3})(\\d{2})(\\d{2})(\\d{2})', f)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            day_of_year = int(match.group(2))\n",
    "            hour = int(match.group(3))\n",
    "            minute = int(match.group(4))\n",
    "            second = int(match.group(5))\n",
    "            dt = datetime(year, 1, 1) + timedelta(days=day_of_year - 1, hours=hour, minutes=minute, seconds=second)\n",
    "            times.append(dt)\n",
    "        else:\n",
    "            times.append(pd.NaT)\n",
    "\n",
    "    df = pd.DataFrame({\"file\": file_list, \"time\": times}).dropna()\n",
    "    df[\"hour\"] = df[\"time\"].dt.floor(\"H\")\n",
    "\n",
    "    # Keep the earliest file per hour\n",
    "    df_hourly = df.sort_values(\"time\").groupby(\"hour\", as_index=False).first()\n",
    "    return df_hourly[\"file\"].tolist(), df_hourly[\"time\"].tolist()\n",
    "\n",
    "BRFimages_hourly, BRFtimes_hourly = filter_hourly_goes_files(BRFimages_all)\n",
    "CMIimages_hourly, CMItimes_hourly = filter_hourly_goes_files(CMIimages_all)\n",
    "\n",
    "# For merged time alignment\n",
    "merged_datetime_range = pd.to_datetime(BRFtimes_hourly)\n",
    "\n",
    "print('BRF images:',len(BRFimages_hourly))\n",
    "print('CMI images:',len(CMIimages_hourly))\n",
    "print(merged_datetime_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cea14cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Date:  20240718\n",
      "s3://hrrrzarr/sfc/20240718/20240718_10z_anl.zarr/\n",
      "HRRR images: 1\n"
     ]
    }
   ],
   "source": [
    "# Create list of HRRR Images\n",
    "\n",
    "start_date = datetime(int(year), 1, 1) + timedelta(days=int(day_start) - 1)\n",
    "end_date = datetime(int(year), 1, 1) + timedelta(days=int(day_stop) - 2)\n",
    "\n",
    "all_dates = []\n",
    "current_date = start_date\n",
    "\n",
    "while current_date <= end_date:\n",
    "    all_dates.append(current_date.strftime('%Y%m%d'))\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "HRRRimages = []\n",
    "\n",
    "for day in all_dates: #range(day_start,day_stop,1):\n",
    "    print(' Date: ',day)\n",
    "    for hour in hour_list:\n",
    "        hour_str = str(hour).zfill(2)\n",
    "        group_url = f's3://hrrrzarr/sfc/{day}/{day}_{hour_str}z_anl.zarr/'\n",
    "        print(group_url)\n",
    "        HRRRimages.append(group_url)\n",
    "\n",
    "print('HRRR images:',len(HRRRimages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5749f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hrrr_dataset(url):\n",
    "    print('Loading HRRR dataset...')\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    hrrr_ds = xr.open_zarr(url, storage_options={\"anon\": True})\n",
    "\n",
    "    # Step 1: Rename dimensions and assign x and y coordinates directly from chunk_index\n",
    "    hrrr_ds = hrrr_ds.rename({\"projection_x_coordinate\": \"x\",\"projection_y_coordinate\": \"y\"})\n",
    "    hrrr_ds = hrrr_ds.assign_coords({\"x\": chunk_index[\"x\"],\"y\": chunk_index[\"y\"][::-1]})\n",
    "\n",
    "    #Set spatial reference\n",
    "    hrrr_ds = hrrr_ds.rio.write_crs(lambert_crs, inplace=False)\n",
    "    hrrr_ds = hrrr_ds.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "    hrrr_ds = hrrr_ds.set_coords(['x', 'y'])\n",
    "    print('Finished loading HRRR dataset.')\n",
    "    # print(hrrr_ds)\n",
    "    return hrrr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "809040eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing projection transformations...\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "Finished HRRR to GOES projection\n"
     ]
    }
   ],
   "source": [
    "# Create projection transformations\n",
    "\n",
    "#Open sample BRF image\n",
    "brfFile = BRFimages_hourly[0]\n",
    "fp2 = fsspec.open(brfFile)\n",
    "brfImg = xr.open_dataset(fp2.open(), engine=\"h5netcdf\", mask_and_scale=True)\n",
    "imgTime = brfImg.t.item()\n",
    "dt = datetime.utcfromtimestamp(imgTime/1000000000).strftime('%Y-%m-%d %H:%M')\n",
    "day_of_year = datetime.utcfromtimestamp(imgTime/1000000000).timetuple().tm_yday\n",
    "\n",
    "print('Performing projection transformations...')\n",
    "\n",
    "#HRRR Projection\n",
    "lambert_crs = CRS.from_proj4(\n",
    "        \"+proj=lcc +lat_1=38.5 +lat_2=38.5 +lat_0=38.5 +lon_0=-97.5 \"\n",
    "        \"+x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +R=6371229\")\n",
    "\n",
    "chunk_index = xr.open_zarr(\"s3://hrrrzarr/grid/HRRR_chunk_index.zarr\", storage_options={\"anon\": True})\n",
    "\n",
    "#Open sample HRRR image\n",
    "store_url = \"s3://hrrrzarr/sfc/20220719/20220719_23z_anl.zarr/surface/SFCR/surface\"\n",
    "hrrr_ds = load_hrrr_dataset(store_url)\n",
    "\n",
    "#GOES Projection\n",
    "G16_CONUS_bounds = (-3627271.340967355, 1583173.7916531805, 1382771.9477514974, 4589199.764884492)\n",
    "G16_crs = CRS.from_wkt('PROJCS[\"unnamed\",GEOGCS[\"unknown\",DATUM[\"unnamed\",SPHEROID[\"Spheroid\",6378137,298.2572221]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Geostationary_Satellite\"],PARAMETER[\"central_meridian\",-75],PARAMETER[\"satellite_height\",35786023],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],EXTENSION[\"PROJ4\",\"+proj=geos +lon_0=-75 +h=35786023 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs +sweep=x\"]]')\n",
    "left, bottom, right, top = G16_CONUS_bounds\n",
    "width, height = brfImg['BRF1'].shape[1], brfImg['BRF1'].shape[0]\n",
    "transform = from_origin(left, top, (right - left) / width, -(bottom - top) / height)\n",
    "print('Finished HRRR to GOES projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e9fb057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_remote(i, BRFimages, CMIimages, HRRRimages):\n",
    "    imgTime = merged_datetime_range[i]\n",
    "    dt = pd.to_datetime(imgTime, format='%Y-%m-%d %H:%M')  # ensures it's a Timestamp\n",
    "    day_of_year = dt.timetuple().tm_yday\n",
    "\n",
    "    if dt < pd.Timestamp(\"2021-01-01 00:00\"):\n",
    "        print(f\"Skipping bad date: {dt}\")\n",
    "        return None\n",
    "\n",
    "    # --- BRF ---\n",
    "    brfFile = BRFimages[i]\n",
    "    fp = fsspec.open(brfFile)\n",
    "    print('Opening BRF file using fsspec...')\n",
    "    with fp.open() as f:\n",
    "        brfImg = xr.open_dataset(f, engine=\"h5netcdf\", mask_and_scale=True)\n",
    "        print('BRF image before reprojection ------------------------')\n",
    "        print(brfImg)\n",
    "        brf_DQF_reproj = reproject_array_to_match(\n",
    "            data_array=brfImg['DQF'],\n",
    "            crs=G16_crs,\n",
    "            transform=transform,\n",
    "            match_ds=hrrr_ds)\n",
    "        print('BRF image after reprrojection ------------------------')\n",
    "        print(brf_DQF_reproj)\n",
    "        # imgTime = brfImg.t.item()\n",
    "\n",
    "        # --- CMI ---\n",
    "        cmiFile = CMIimages[i]\n",
    "        fp2 = fsspec.open(cmiFile)\n",
    "        print('Opening CMI file using fsspec...')\n",
    "        with fp2.open() as f2:\n",
    "            cmiImg = xr.open_dataset(f2, engine=\"h5netcdf\", mask_and_scale=True)\n",
    "            print('CMI image before reprojection ------------------------')\n",
    "            print(cmiImg)\n",
    "            # stack 16 CMI bands\n",
    "            cmi_reprojected_bands = []\n",
    "            for band_num in range(1, 17):\n",
    "                band_str = str(band_num).zfill(2)\n",
    "                cmi_band = cmiImg[f'CMI_C{band_str}']\n",
    "                cmi_reproj = reproject_array_to_match(\n",
    "                    data_array=cmi_band,\n",
    "                    crs=G16_crs,\n",
    "                    transform=transform,\n",
    "                    match_ds=hrrr_ds)\n",
    "                cmi_reprojected_bands.append(cmi_reproj.data)\n",
    "            cmi_stack = np.stack(cmi_reprojected_bands, axis=-1).squeeze()\n",
    "            print('CMI image after reprrojection ------------------------')\n",
    "            print(cmi_stack)\n",
    "\n",
    "            # --- Solar + DOY ---\n",
    "            print('Performing solar calculations...')\n",
    "            SZA_SAA = calcSolar(cmi_reproj, dt)\n",
    "            SZAarray, SAAarray = SZA_SAA\n",
    "            DOY = np.full((1059, 1799), day_of_year, dtype=np.int16)\n",
    "\n",
    "            # --- DSR ---\n",
    "            print('Creating DSR array...')\n",
    "            DSRpredictArray = np.zeros([1059, 1799, 8])\n",
    "            DSRbandIndices = [b - 1 for b in [1, 2, 4, 6, 12, 15]]\n",
    "            DSRpredictArray[:, :, 0:6] = cmi_stack[:, :, DSRbandIndices]\n",
    "            DSRpredictArray[:, :, 6] = SZAarray\n",
    "            DSRpredictArray[:, :, 7] = SAAarray\n",
    "            print('Predicting DSR...')\n",
    "            dsrArr = DSRmodel.predict(np.nan_to_num(DSRpredictArray.reshape(-1, 8)))\n",
    "            dsrImg = dsrArr.reshape(1059, 1799)\n",
    "            print(\"DSR image predicted\")\n",
    "\n",
    "            # --- LST ---\n",
    "            print('Creating LST array...')\n",
    "            LSTpredictArray = np.zeros([1059, 1799, 12])\n",
    "            LSTbandIndices = [b - 1 for b in [7, 10, 11, 12, 13, 14, 15, 16]]\n",
    "            LSTpredictArray[:, :, 0:8] = cmi_stack[:, :, LSTbandIndices]\n",
    "            LSTpredictArray[:, :, 8] = SZAarray\n",
    "            LSTpredictArray[:, :, 9] = SAAarray\n",
    "            LSTpredictArray[:, :, 10] = DOY\n",
    "            LSTpredictArray[:, :, 11] = dsrImg\n",
    "            print('Predicting LST...')\n",
    "            lstArr = LSTmodel.predict(np.nan_to_num(LSTpredictArray.reshape(-1, 12)))\n",
    "            lstImg = lstArr.reshape(1059, 1799)\n",
    "            print(\"LST image predicted\")\n",
    "\n",
    "            # --- ET ---\n",
    "            print('Creating ET array...')\n",
    "            ETpredictArray = np.zeros([1059, 1799, 30])\n",
    "            brfBands = ['1', '2', '3', '5', '6']\n",
    "            print('Reprojecting BRF images to match ET array...')\n",
    "            for k, b in enumerate(brfBands):\n",
    "                brf_band = brfImg[f'BRF{b}']\n",
    "                brf_reproj = reproject_array_to_match(\n",
    "                    data_array=brf_band,\n",
    "                    crs=G16_crs,\n",
    "                    transform=transform,\n",
    "                    match_ds=hrrr_ds)\n",
    "                ETpredictArray[:, :, k] = brf_reproj.data\n",
    "\n",
    "            print('Assigning BRF, DSR, LST variables...')\n",
    "            BRF2, BRF3, BRF6 = ETpredictArray[:, :, 1], ETpredictArray[:, :, 2], ETpredictArray[:, :, 4]\n",
    "            DSR, LST = dsrImg, lstImg\n",
    "\n",
    "            print('Calculating NIRvP and sNIRvP')\n",
    "            NIRvP = ((BRF3 - BRF2) / (BRF3 + BRF2)) * BRF3 * DSR\n",
    "            sNIRvP = ((BRF3 - BRF2) / (BRF3 + BRF2)) * ((BRF3 - BRF6) / (BRF3 + BRF6)) * BRF3 * DSR\n",
    "            print('NIRvP:',NIRvP,'sNIRvP:',sNIRvP)\n",
    "\n",
    "            ETpredictArray[:, :, 5] = DSR\n",
    "            ETpredictArray[:, :, 6] = LST\n",
    "            ETpredictArray[:, :, 7] = NIRvP\n",
    "            ETpredictArray[:, :, 8] = sNIRvP\n",
    "            ETpredictArray[:, :, 9:16] = cmi_stack[:, :, [b - 1 for b in [1, 3, 12, 13, 14, 15, 16]]]\n",
    "\n",
    "            # HRRR vars\n",
    "            print('Loading and assigning HRRR data...')\n",
    "            for n in range(16, 30):\n",
    "                print(ET_hrrr_paths[n - 16])\n",
    "                ds = load_hrrr_dataset(HRRRimages[i] + ET_hrrr_paths[n - 16])\n",
    "                ETpredictArray[:, :, n] = ds[ET_hrrr_vars[n - 16]].data\n",
    "\n",
    "            print('Reshaping ET array and prediction...')\n",
    "            reshapedArr = np.reshape(ETpredictArray, (ETpredictArray.shape[0] * ETpredictArray.shape[1], ETpredictArray.shape[2]))\n",
    "            maskedArr = np.nan_to_num(reshapedArr)\n",
    "            etArr = ETmodel.predict(maskedArr).astype(np.float16)\n",
    "            etImg = etArr.reshape(ETpredictArray[:, :, 0].shape)\n",
    "            print(f\"ET image {i} predicted\")\n",
    "\n",
    "            print('Creating DQF mask...')\n",
    "            etImg_masked = np.where((brf_DQF_reproj[0] > 16) | np.isnan(brf_DQF_reproj[0]), 0, etImg)\n",
    "\n",
    "            print('Creating xr dataset...')\n",
    "            ds_ALIVE_ET = xr.Dataset({\n",
    "                \"ET\": ((\"y\", \"x\"), etImg_masked.astype(np.float32)),\n",
    "                \"ET_DQF\": ((\"y\", \"x\"), np.nan_to_num(brf_DQF_reproj[0], nan=255).astype(np.uint8))})\n",
    "            print('Mean', np.mean(ds_ALIVE_ET))\n",
    "\n",
    "            timestamp = pd.Timestamp(dt)\n",
    "            print('Writing to timestamp ',timestamp)\n",
    "            ds_to_write = ds_ALIVE_ET.expand_dims(dim={\"t\": [timestamp]}).reset_coords(drop=True)\n",
    "            # ds_to_write = ds_ALIVE_ET.expand_dims(\"t\").reset_coords(drop=True)\n",
    "            \n",
    "            ds_to_write = ds_to_write.assign_coords(t=(\"t\", [pd.Timestamp(dt)]))\n",
    "            print('ET Array ------------------------------')\n",
    "            print(ds_to_write)\n",
    "            return(ds_to_write)\n",
    "            \n",
    "            # to_icechunk(ds_to_write, session, group=\"ET\", region='auto') # mode=\"a\", append_dim=\"t\")\n",
    "            # return(session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67f8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPP ---\n",
    "                # GPPpredictArray = np.zeros([1059, 1799, 9])\n",
    "                # for k, b in enumerate(brfBands):\n",
    "                #     GPPpredictArray[:, :, k] = brfImg[f'BRF{b}'].data\n",
    "\n",
    "                # BRF2_gpp, BRF3_gpp, BRF6_gpp = GPPpredictArray[:, :, 1], GPPpredictArray[:, :, 2], GPPpredictArray[:, :, 4]\n",
    "\n",
    "                # GPPpredictArray[:, :, 5] = NIRvP\n",
    "                # GPPpredictArray[:, :, 6] = sNIRvP\n",
    "                # GPPpredictArray[:, :, 7] = DSR\n",
    "                # GPPpredictArray[:, :, 8] = LST\n",
    "\n",
    "                # gppArr = GPPmodel.predict(np.nan_to_num(GPPpredictArray.reshape(-1, 9))).astype(np.float16)\n",
    "                # gppImg = gppArr.reshape(1059, 1799)\n",
    "                # gppImg_masked = np.where(brfImg['DQF'] > 16, 0, gppImg)\n",
    "\n",
    "                # ds_ALIVE_GPP = xr.Dataset({\n",
    "                #     \"GPP\": ((\"y\", \"x\"), gppImg_masked.astype(np.float32)),\n",
    "                #     \"GPP_DQF\": ((\"y\", \"x\"), brfImg[\"DQF\"].astype(np.uint8).data),\n",
    "                # })\n",
    "                # ds_ALIVE_GPP[\"t\"] = pd.Timestamp(dt)\n",
    "\n",
    "                # print(\"GPP predicted\")\n",
    "                # return ds_to_write#, ds_ALIVE_GPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_arraylake_group(variable):\n",
    "    # Use HRRR as grid reference\n",
    "    y_coords, x_coords = hrrr_ds.y.astype(\"float32\"), hrrr_ds.x.astype(\"float32\")\n",
    "    ny, nx = len(y_coords), len(x_coords)\n",
    "    nt = len(merged_datetime_range)\n",
    "    print('New dates: ', nt)\n",
    "    \n",
    "    init_ds = xr.Dataset(\n",
    "        {variable: ((\"t\", \"y\", \"x\"), dask.array.full((nt, ny, nx), np.nan, dtype=np.float16, chunks=(1, ny, nx))),\n",
    "         variable + \"_DQF\": ((\"t\", \"y\", \"x\"), dask.array.zeros((nt, ny, nx), dtype=np.uint8, chunks=(1, ny, nx)))},\n",
    "         coords={\"t\": merged_datetime_range, \"y\": y_coords, \"x\": x_coords},\n",
    "    )\n",
    "    init_ds = init_ds.reset_coords(names=[\"spatial_ref\"], drop=True)\n",
    "\n",
    "    # Initialize Arraylake group once for entire dataset (year)\n",
    "    session = repo.writable_session(branch=\"main\")\n",
    "    # to_icechunk(init_ds, session, group=variable, region=None, mode=\"w\") # for brand new or full write-over\n",
    "\n",
    "    init_ds.to_zarr(session.store, compute=False, group=variable, mode=\"w\", consolidated=False)\n",
    "    session.commit(\"initialize \" + variable + \" store\")\n",
    "\n",
    "    print(\" Dummy dataset initialized for \" + variable)\n",
    "\n",
    "# initialize_arraylake_group('ET')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ef7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening BRF file using fsspec...\n",
      "BRF image before reprojection ------------------------\n",
      "<xarray.Dataset> Size: 90MB\n",
      "Dimensions:                                 (y: 1500, x: 2500,\n",
      "                                             number_of_time_bounds: 2,\n",
      "                                             number_of_image_bounds: 2,\n",
      "                                             number_of_LZA_bounds: 2,\n",
      "                                             number_of_SZA_bounds: 2)\n",
      "Coordinates:\n",
      "  * y                                       (y) float32 6kB 0.1282 ... 0.04427\n",
      "  * x                                       (x) float32 10kB -0.1013 ... 0.03861\n",
      "    t                                       datetime64[ns] 8B ...\n",
      "    y_image                                 float32 4B ...\n",
      "    x_image                                 float32 4B ...\n",
      "    retrieval_local_zenith_angle            float32 4B ...\n",
      "    quantitative_local_zenith_angle         float32 4B ...\n",
      "    retrieval_solar_zenith_angle            float32 4B ...\n",
      "    quantitative_solar_zenith_angle         float32 4B ...\n",
      "Dimensions without coordinates: number_of_time_bounds, number_of_image_bounds,\n",
      "                                number_of_LZA_bounds, number_of_SZA_bounds\n",
      "Data variables: (12/54)\n",
      "    BRF1                                    (y, x) float32 15MB ...\n",
      "    BRF2                                    (y, x) float32 15MB ...\n",
      "    BRF3                                    (y, x) float32 15MB ...\n",
      "    BRF5                                    (y, x) float32 15MB ...\n",
      "    BRF6                                    (y, x) float32 15MB ...\n",
      "    DQF                                     (y, x) float32 15MB ...\n",
      "    ...                                      ...\n",
      "    granule_level_quality_flag              float64 8B ...\n",
      "    algorithm_dynamic_input_data_container  int32 4B ...\n",
      "    processing_parm_version_container       int32 4B ...\n",
      "    algorithm_product_version_container     int32 4B ...\n",
      "    percent_uncorrectable_GRB_errors        float32 4B ...\n",
      "    percent_uncorrectable_L0_errors         float32 4B ...\n",
      "Attributes: (12/29)\n",
      "    naming_authority:          gov.nesdis.noaa\n",
      "    Conventions:               CF-1.7\n",
      "    Metadata_Conventions:      Unidata Dataset Discovery v1.0\n",
      "    standard_name_vocabulary:  CF Standard Name Table (v35, 20 July 2016)\n",
      "    institution:               DOC/NOAA/NESDIS > U.S. Department of Commerce,...\n",
      "    project:                   GOES\n",
      "    ...                        ...\n",
      "    cdm_data_type:             Image\n",
      "    time_coverage_start:       2024-07-18T10:01:18.0Z\n",
      "    time_coverage_end:         2024-07-18T10:03:55.3Z\n",
      "    timeline_id:               ABI Mode 6\n",
      "    production_data_source:    Realtime\n",
      "    id:                        2a3c8d59-2766-4ccd-abd7-1a53f82a07e0\n",
      "BRF image after reprrojection ------------------------\n",
      "<xarray.DataArray 'data_band' (band: 1, y: 1059, x: 1799)> Size: 8MB\n",
      "array([[[25., 25., 25., ..., 25., 25., 25.],\n",
      "        [25., 25., 25., ..., 25., 25., 25.],\n",
      "        [25., 25., 25., ..., 25., 25., 25.],\n",
      "        ...,\n",
      "        [nan, nan, nan, ..., 25., 25., 25.],\n",
      "        [nan, nan, nan, ..., 25., 25., 25.],\n",
      "        [nan, nan, nan, ..., 25., 25., 25.]]],\n",
      "      shape=(1, 1059, 1799), dtype=float32)\n",
      "Coordinates:\n",
      "  * band         (band) int64 8B 1\n",
      "    spatial_ref  int64 8B 0\n",
      "  * x            (x) float64 14kB -2.698e+06 -2.695e+06 ... 2.693e+06 2.696e+06\n",
      "  * y            (y) float64 8kB 1.587e+06 1.584e+06 ... -1.584e+06 -1.587e+06\n",
      "Attributes:\n",
      "    _FillValue:  nan\n",
      "Opening CMI file using fsspec...\n",
      "CMI image before reprojection ------------------------\n",
      "<xarray.Dataset> Size: 480MB\n",
      "Dimensions:                                 (y: 1500, x: 2500,\n",
      "                                             number_of_time_bounds: 2,\n",
      "                                             number_of_image_bounds: 2, band: 1)\n",
      "Coordinates: (12/37)\n",
      "    t                                       datetime64[ns] 8B ...\n",
      "  * y                                       (y) float32 6kB 0.1282 ... 0.04427\n",
      "  * x                                       (x) float32 10kB -0.1013 ... 0.03861\n",
      "    y_image                                 float32 4B ...\n",
      "    x_image                                 float32 4B ...\n",
      "    band_wavelength_C01                     (band) float32 4B ...\n",
      "    ...                                      ...\n",
      "    band_id_C11                             (band) int8 1B ...\n",
      "    band_id_C12                             (band) int8 1B ...\n",
      "    band_id_C13                             (band) int8 1B ...\n",
      "    band_id_C14                             (band) int8 1B ...\n",
      "    band_id_C15                             (band) int8 1B ...\n",
      "    band_id_C16                             (band) int8 1B ...\n",
      "Dimensions without coordinates: number_of_time_bounds, number_of_image_bounds,\n",
      "                                band\n",
      "Data variables: (12/124)\n",
      "    CMI_C01                                 (y, x) float32 15MB ...\n",
      "    DQF_C01                                 (y, x) float32 15MB ...\n",
      "    CMI_C02                                 (y, x) float32 15MB ...\n",
      "    DQF_C02                                 (y, x) float32 15MB ...\n",
      "    CMI_C03                                 (y, x) float32 15MB ...\n",
      "    DQF_C03                                 (y, x) float32 15MB ...\n",
      "    ...                                      ...\n",
      "    mean_brightness_temperature_C16         float32 4B ...\n",
      "    std_dev_brightness_temperature_C16      float32 4B ...\n",
      "    percent_uncorrectable_GRB_errors        float32 4B ...\n",
      "    percent_uncorrectable_L0_errors         float32 4B ...\n",
      "    dynamic_algorithm_input_data_container  int32 4B ...\n",
      "    algorithm_product_version_container     int32 4B ...\n",
      "Attributes: (12/29)\n",
      "    naming_authority:          gov.nesdis.noaa\n",
      "    Conventions:               CF-1.7\n",
      "    Metadata_Conventions:      Unidata Dataset Discovery v1.0\n",
      "    standard_name_vocabulary:  CF Standard Name Table (v35, 20 July 2016)\n",
      "    institution:               DOC/NOAA/NESDIS > U.S. Department of Commerce,...\n",
      "    project:                   GOES\n",
      "    ...                        ...\n",
      "    date_created:              2024-07-18T10:04:08.3Z\n",
      "    time_coverage_start:       2024-07-18T10:01:18.0Z\n",
      "    time_coverage_end:         2024-07-18T10:03:55.3Z\n",
      "    timeline_id:               ABI Mode 6\n",
      "    production_data_source:    Realtime\n",
      "    id:                        51e96a9b-02bf-4a42-a54f-5ec840348c86\n",
      "CMI image after reprrojection ------------------------\n",
      "[[[0.0000000e+00 0.0000000e+00 3.1746001e-04 ... 2.4587672e+02\n",
      "   2.4473346e+02 2.3850081e+02]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.4773210e+02\n",
      "   2.4646072e+02 2.3965752e+02]\n",
      "  [0.0000000e+00 3.1746001e-04 0.0000000e+00 ... 2.4473956e+02\n",
      "   2.4348270e+02 2.3800508e+02]\n",
      "  ...\n",
      "  [1.8380935e-01 1.5111096e-01 1.8031728e-01 ... 2.4701389e+02\n",
      "   2.4532907e+02 2.4164046e+02]\n",
      "  [2.0317441e-01 1.7079349e-01 2.0317441e-01 ... 2.4635553e+02\n",
      "   2.4503128e+02 2.4175063e+02]\n",
      "  [2.0825377e-01 1.7206332e-01 2.0857123e-01 ... 2.4635553e+02\n",
      "   2.4515039e+02 2.4164046e+02]]\n",
      "\n",
      " [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.4946777e+02\n",
      "   2.4800931e+02 2.4081424e+02]\n",
      "  [0.0000000e+00 0.0000000e+00 3.1746001e-04 ... 2.4587672e+02\n",
      "   2.4473346e+02 2.3850081e+02]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.4444031e+02\n",
      "   2.4342313e+02 2.3783983e+02]\n",
      "  ...\n",
      "  [1.9301569e-01 1.6063477e-01 1.9142839e-01 ... 2.4695404e+02\n",
      "   2.4544821e+02 2.4175063e+02]\n",
      "  [2.0190457e-01 1.6888873e-01 2.0317441e-01 ... 2.4641537e+02\n",
      "   2.4515039e+02 2.4175063e+02]\n",
      "  [2.0507917e-01 1.7079349e-01 2.0539662e-01 ... 2.4737300e+02\n",
      "   2.4592468e+02 2.4224635e+02]]\n",
      "\n",
      " [[0.0000000e+00 0.0000000e+00 3.1746001e-04 ... 2.4539792e+02\n",
      "   2.4407831e+02 2.3850081e+02]\n",
      "  [0.0000000e+00 3.1746001e-04 0.0000000e+00 ... 2.4456001e+02\n",
      "   2.4360181e+02 2.3817032e+02]\n",
      "  [0.0000000e+00 0.0000000e+00 3.1746001e-04 ... 2.4587672e+02\n",
      "   2.4473346e+02 2.3850081e+02]\n",
      "  ...\n",
      "  [2.0126964e-01 1.6825381e-01 2.0222203e-01 ... 2.4671463e+02\n",
      "   2.4544821e+02 2.4208112e+02]\n",
      "  [2.1047600e-01 1.7777760e-01 2.1174583e-01 ... 2.4677449e+02\n",
      "   2.4556732e+02 2.4191586e+02]\n",
      "  [2.1111090e-01 1.7872998e-01 2.1492043e-01 ... 2.4707375e+02\n",
      "   2.4568643e+02 2.4208112e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  [          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  [          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  ...\n",
      "  [0.0000000e+00 6.3492003e-04 9.5238001e-04 ... 2.9214136e+02\n",
      "   2.8898715e+02 2.7435889e+02]\n",
      "  [0.0000000e+00 3.1746001e-04 6.3492003e-04 ... 2.9297925e+02\n",
      "   2.8952319e+02 2.7474445e+02]\n",
      "  [3.1746001e-04 3.1746001e-04 6.3492003e-04 ... 2.9262018e+02\n",
      "   2.8916583e+02 2.7435889e+02]]\n",
      "\n",
      " [[          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  [          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  [          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  ...\n",
      "  [0.0000000e+00 6.3492003e-04 3.1746001e-04 ... 2.9262018e+02\n",
      "   2.8922540e+02 2.7452414e+02]\n",
      "  [0.0000000e+00 3.1746001e-04 0.0000000e+00 ... 2.9285956e+02\n",
      "   2.8928497e+02 2.7452414e+02]\n",
      "  [3.1746001e-04 6.3492003e-04 0.0000000e+00 ... 2.9285956e+02\n",
      "   2.8922540e+02 2.7435889e+02]]\n",
      "\n",
      " [[          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  [          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  [          nan           nan           nan ...           nan\n",
      "             nan           nan]\n",
      "  ...\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.9291943e+02\n",
      "   2.8940408e+02 2.7452414e+02]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 2.9357776e+02\n",
      "   2.8982101e+02 2.7474445e+02]\n",
      "  [0.0000000e+00 6.3492003e-04 0.0000000e+00 ... 2.9345807e+02\n",
      "   2.8970187e+02 2.7463428e+02]]]\n",
      "Performing solar calculations...\n",
      "Performing solar calculations...\n",
      "Finding latitude and lingutude of array point...\n",
      "Latitude: [[36.55156675 36.55732381 36.56307464 ... 36.56506735 36.55931868\n",
      "  36.55356378]\n",
      " [36.52572861 36.53148489 36.53723494 ... 36.53922738 36.53347949\n",
      "  36.52772537]\n",
      " [36.49988756 36.50564306 36.51139233 ... 36.51338451 36.50763739\n",
      "  36.50188405]\n",
      " ...\n",
      " [ 8.78778646  8.7923245   8.79685774 ...  8.79842857  8.793897\n",
      "   8.78936062]\n",
      " [ 8.7620899   8.76662653  8.77115836 ...  8.77272871  8.76819854\n",
      "   8.76366357]\n",
      " [ 8.73639629  8.7409315   8.74546193 ...  8.74703178  8.74250302\n",
      "   8.73796947]] Longitude: [[-124.73913785 -124.7071131  -124.67508498 ...  -65.33602193\n",
      "   -65.30399264  -65.27196673]\n",
      " [-124.73200013 -124.69998257 -124.66796164 ...  -65.34314277\n",
      "   -65.31112068  -65.27910195]\n",
      " [-124.72486578 -124.69285541 -124.66084167 ...  -65.35026026\n",
      "   -65.31824535  -65.28623381]\n",
      " ...\n",
      " [-118.70900824 -118.68317412 -118.65733827 ...  -71.35162101\n",
      "   -71.32578456  -71.29994985]\n",
      " [-118.70444477 -118.67861543 -118.65278435 ...  -71.35617328\n",
      "   -71.3303416   -71.30451166]\n",
      " [-118.69988305 -118.67405847 -118.64823216 ...  -71.36072382\n",
      "   -71.3348969   -71.30907173]]\n",
      "Finished solar calculations.\n",
      "Creating DSR array...\n",
      "Predicting DSR...\n",
      "DSR image predicted\n",
      "Creating LST array...\n",
      "Predicting LST...\n",
      "LST image predicted\n",
      "Creating ET array...\n",
      "Reprojecting BRF images to match ET array...\n",
      "Assigning BRF, DSR, LST variables...\n",
      "Calculating NIRvP and sNIRvP\n",
      "NIRvP: [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] sNIRvP: [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "Loading and assigning HRRR data...\n",
      "surface/SFCR/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/FRICV/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/LHTFL/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/CAPE/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/ULWRF/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/USWRF/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/DSWRF/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/VDDSF/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/HPBL/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/GFLUX/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "surface/TMP/surface\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "2m_above_ground/SPFH/2m_above_ground\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "2m_above_ground/RH/2m_above_ground\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "2m_above_ground/TMP/2m_above_ground\n",
      "Loading HRRR dataset...\n",
      "Finished loading HRRR dataset.\n",
      "Reshaping ET array and prediction...\n",
      "ET image 0 predicted\n",
      "Creating DQF mask...\n",
      "Creating xr dataset...\n",
      "Mean <xarray.Dataset> Size: 12B\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    ET       float32 4B 0.0\n",
      "    ET_DQF   float64 8B 42.18\n",
      "Writing to timestamp  2024-07-18 10:01:18\n",
      "ET Array ------------------------------\n",
      "<xarray.Dataset> Size: 10MB\n",
      "Dimensions:  (t: 1, y: 1059, x: 1799)\n",
      "Coordinates:\n",
      "  * t        (t) datetime64[ns] 8B 2024-07-18T10:01:18\n",
      "Dimensions without coordinates: y, x\n",
      "Data variables:\n",
      "    ET       (t, y, x) float32 8MB 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0\n",
      "    ET_DQF   (t, y, x) uint8 2MB 25 25 25 25 25 25 25 ... 25 25 25 25 25 25 25\n"
     ]
    }
   ],
   "source": [
    "# def worker(i):\n",
    "for i in range(len(BRFimages_hourly)):\n",
    "    process_image_remote(i, BRFimages_hourly, CMIimages_hourly, HRRRimages)\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=24) as executor:\n",
    "#     futures = [executor.submit(worker, i) for i in range(len(BRFimages_hourly))]\n",
    "#     wait(futures)\n",
    "\n",
    "#### END OF CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ae6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Successfully refreshed tokens!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32m Successfully refreshed tokens!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&gt; Token stored at </span><span style=\"color: #bf7fbf; text-decoration-color: #bf7fbf\">/Users/sophie/.arraylake/</span><span style=\"color: #ff7fff; text-decoration-color: #ff7fff\">token.json</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m> Token stored at \u001b[0m\u001b[2;35m/Users/sophie/.arraylake/\u001b[0m\u001b[2;95mtoken.json\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  User Details \n",
       " Name: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">SOPHIE HOFFMAN</span>                                                                                            \n",
       " Email: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">shoffman22@wisc.edu</span>                                                                                      \n",
       " Id: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">b020a038-c0bd-45c5-a6f1-b9796432b6e4</span>                                                                        \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  User Details \n",
       " Name: \u001b[2mSOPHIE HOFFMAN\u001b[0m                                                                                            \n",
       " Email: \u001b[2mshoffman22@wisc.edu\u001b[0m                                                                                      \n",
       " Id: \u001b[2mb020a038-c0bd-45c5-a6f1-b9796432b6e4\u001b[0m                                                                        \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Successfully logged in!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m Successfully logged in!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&gt; Token stored at </span><span style=\"color: #bf7fbf; text-decoration-color: #bf7fbf\">/Users/sophie/.arraylake/</span><span style=\"color: #ff7fff; text-decoration-color: #ff7fff\">token.json</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m> Token stored at \u001b[0m\u001b[2;35m/Users/sophie/.arraylake/\u001b[0m\u001b[2;95mtoken.json\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  User Details \n",
       " Name: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">SOPHIE HOFFMAN</span>                                                                                            \n",
       " Email: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">shoffman22@wisc.edu</span>                                                                                      \n",
       " Id: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">b020a038-c0bd-45c5-a6f1-b9796432b6e4</span>                                                                        \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "  User Details \n",
       " Name: \u001b[2mSOPHIE HOFFMAN\u001b[0m                                                                                            \n",
       " Email: \u001b[2mshoffman22@wisc.edu\u001b[0m                                                                                      \n",
       " Id: \u001b[2mb020a038-c0bd-45c5-a6f1-b9796432b6e4\u001b[0m                                                                        \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[133]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Committed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m([r\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mr\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mresults\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mr\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m datasets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[133]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m process_image_remote(args)  \u001b[38;5;66;03m# returns ds_to_write\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers=\u001b[32m4\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBRFimages_hourly\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCMIimages_hourly\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHRRRimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# repo_str = 'wisc-alive/ALIVE_v3-icechunk-lightweight-model-test'\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# args = [(i, BRFimages_hourly, CMIimages_hourly, HRRRimages) for i in range(len(BRFimages_hourly))]\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# results = process_image_remote.map(args)\u001b[39;00m\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#  Merge and commit all forked sessions\u001b[39;00m\n\u001b[32m     22\u001b[39m session.merge(*results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[133]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mmain.<locals>.worker\u001b[39m\u001b[34m(i)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mworker\u001b[39m(i):\n\u001b[32m     11\u001b[39m     args = [(i, BRFimages_hourly, CMIimages_hourly, HRRRimages)]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_image_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[128]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mprocess_image_remote\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_image_remote\u001b[39m(args):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     i, BRFimages, CMIimages, HRRRimages = args\n\u001b[32m     21\u001b[39m     repo = aclient.get_repo(\u001b[33m'\u001b[39m\u001b[33mwisc-alive/ALIVE_v3-icechunk-lightweight-model-test\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m     session = repo.writable_session(branch=\u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m).fork()\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # aclient = al.Client()\n",
    "    # aclient.login()\n",
    "    # repo = aclient.get_repo('wisc-alive/ALIVE_v3-icechunk-lightweight-model-test')\n",
    "    # results = []\n",
    "\n",
    "    session = repo.writable_session(branch=\"main\")\n",
    "    # forked_session = session.fork()\n",
    "\n",
    "    def worker(i):\n",
    "        args = [(i, BRFimages_hourly, CMIimages_hourly, HRRRimages)]\n",
    "        return process_image_remote(args)  # returns ds_to_write\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as ex:\n",
    "        results = list(ex.map(worker, range(min(len(BRFimages_hourly), len(CMIimages_hourly), len(HRRRimages)))))\n",
    "    \n",
    "    # repo_str = 'wisc-alive/ALIVE_v3-icechunk-lightweight-model-test'\n",
    "    args = [(i, BRFimages_hourly, CMIimages_hourly, HRRRimages) for i in range(len(BRFimages_hourly))]\n",
    "    results = process_image_remote.map(args)\n",
    "\n",
    "    #  Merge and commit all forked sessions\n",
    "    session.merge(*results)\n",
    "\n",
    "    session.commit(f\"ALIVE upload for {year_str}-{day_str}\")\n",
    "    print(f\" Committed {len([r for r in results if r is not None])} datasets\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4e501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Group 'ET' successfully loaded.\n",
      "   Variables: ['ET', 'ET_DQF']\n",
      "   Dimensions: {'t': 69012, 'y': 1059, 'x': 1799}\n",
      "   Time range: 2024-01-01T00:01:17.000000000  2024-12-30T23:56:17.000000000\n",
      "   Number of timesteps: 69012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_repo_contents(repo, group=\"ET\"):\n",
    "    # Open a read-only session on the main branch\n",
    "    repo = aclient.get_repo('wisc-alive/ALIVE_v3-icechunk-lightweight-model-test')\n",
    "\n",
    "    # first create a session\n",
    "    session = repo.readonly_session(branch=\"main\")\n",
    "    try:\n",
    "        # Open the group as an xarray dataset\n",
    "        ds = xr.open_zarr(session.store, group=group, consolidated=False)\n",
    "\n",
    "        print(f\" Group '{group}' successfully loaded.\")\n",
    "        print(f\"   Variables: {list(ds.data_vars)}\")\n",
    "        print(f\"   Dimensions: {dict(ds.dims)}\")\n",
    "        print(f\"   Time range: {ds['t'].min().values}  {ds['t'].max().values}\")\n",
    "        print(f\"   Number of timesteps: {len(ds['t'])}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Could not open group '{group}': {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "check_repo_contents(repo, group=\"ET\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b37ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def visualize_icechunk_data(repo, group, start_time, end_time, variable=None):\n",
    "    \"\"\"\n",
    "    Visualize data from an Icechunk repo over a time period.\n",
    "    \"\"\"\n",
    "    # --- Open dataset from Icechunk ---\n",
    "    repo = aclient.get_repo('wisc-alive/ALIVE_v3-icechunk-lightweight-model-test')\n",
    "    session = repo.readonly_session(branch=\"main\")\n",
    "    ds = xr.open_zarr(session.store, group=group, consolidated=False)\n",
    "\n",
    "    # Normalize time coordinate\n",
    "    if not np.issubdtype(ds.t.dtype, np.datetime64):\n",
    "        ds['t'] = pd.to_datetime(ds['t'].values)\n",
    "\n",
    "    # Default to first variable if not specified\n",
    "    if variable is None:\n",
    "        variable = [v for v in ds.data_vars.keys() if not v.endswith(\"_DQF\")][0]\n",
    "\n",
    "    # --- Select time range ---\n",
    "    start_time = pd.to_datetime(start_time)\n",
    "    end_time = pd.to_datetime(end_time)\n",
    "    ds_time = ds.sel(t=slice(start_time, end_time))\n",
    "\n",
    "    if ds_time[variable].size == 0:\n",
    "        print(f\" No data found between {start_time} and {end_time}\")\n",
    "        print(f\"Available times: {ds.t.values[:5]} ...\")\n",
    "        return\n",
    "\n",
    "    # --- Compute spatial mean over time ---\n",
    "    mean_series = ds_time[variable].mean(dim=[\"x\", \"y\"])\n",
    "\n",
    "    # --- Plot time series ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    mean_series.plot(marker='o', linewidth=1)\n",
    "    plt.title(f\"Temporal Mean of {variable} ({start_time:%Y-%m-%d}  {end_time:%Y-%m-%d})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(variable)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot a sample map safely (use nearest match) ---\n",
    "    mid_time = start_time + (end_time - start_time) / 2\n",
    "    nearest_t = ds.t.sel(t=mid_time, method=\"nearest\").item()\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    ds[variable].sel(t=nearest_t, method=\"nearest\").plot(cmap=\"viridis\")\n",
    "    plt.title(f\"{variable} near {pd.to_datetime(str(nearest_t)):%Y-%m-%d %H:%M}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\" Displayed {len(ds_time.t)} timesteps from {start_time} to {end_time} for {variable}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d11ed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of timestamps: 48\n",
      "Unique timestamps: 32\n",
      "['2024-01-02T00:01:17.000000000' '2024-01-02T01:01:17.000000000'\n",
      " '2024-01-02T10:01:17.000000000' '2024-01-02T11:01:17.000000000'\n",
      " '2024-01-02T12:01:17.000000000' '2024-01-02T13:01:17.000000000'\n",
      " '2024-01-02T14:01:17.000000000' '2024-01-02T15:01:17.000000000'\n",
      " '2024-01-02T16:01:17.000000000' '2024-01-02T17:01:17.000000000']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Cannot get left slice bound for non-unique label: Timestamp('2024-01-02 00:01:17')\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m duplicates = ds.t.to_series().duplicated(keep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(ds.t.values[duplicates.values][:\u001b[32m10\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mvisualize_icechunk_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_datetime_range\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmerged_datetime_range\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[136]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mvisualize_icechunk_data\u001b[39m\u001b[34m(repo, group, start_time, end_time, variable)\u001b[39m\n\u001b[32m     24\u001b[39m start_time = pd.to_datetime(start_time)\n\u001b[32m     25\u001b[39m end_time = pd.to_datetime(end_time)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m ds_time = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43msel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ds_time[variable].size == \u001b[32m0\u001b[39m:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m No data found between \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/site-packages/xarray/core/dataset.py:2991\u001b[39m, in \u001b[36mDataset.sel\u001b[39m\u001b[34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[39m\n\u001b[32m   2923\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns a new dataset with each array indexed by tick labels\u001b[39;00m\n\u001b[32m   2924\u001b[39m \u001b[33;03malong the specified dimension(s).\u001b[39;00m\n\u001b[32m   2925\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2988\u001b[39m \n\u001b[32m   2989\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2990\u001b[39m indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \u001b[33m\"\u001b[39m\u001b[33msel\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2991\u001b[39m query_results = \u001b[43mmap_index_queries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2992\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtolerance\u001b[49m\n\u001b[32m   2993\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m drop:\n\u001b[32m   2996\u001b[39m     no_scalar_variables = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/site-packages/xarray/core/indexing.py:201\u001b[39m, in \u001b[36mmap_index_queries\u001b[39m\u001b[34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m         results.append(IndexSelResult(labels))\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         results.append(\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43msel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    203\u001b[39m merged = merge_sel_results(results)\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# drop dimension coordinates found in dimension indexers\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# (also drop multi-index if any)\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# (.sel() already ensures alignment)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/site-packages/xarray/core/indexes.py:835\u001b[39m, in \u001b[36mPandasIndex.sel\u001b[39m\u001b[34m(self, labels, method, tolerance)\u001b[39m\n\u001b[32m    832\u001b[39m coord_name, label = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(labels.items()))\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m     indexer = \u001b[43m_query_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_dict_like(label):\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    838\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcannot use a dict-like object for selection on \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    839\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33ma dimension that does not have a MultiIndex\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    840\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/site-packages/xarray/core/indexes.py:567\u001b[39m, in \u001b[36m_query_slice\u001b[39m\u001b[34m(index, label, coord_name, method, tolerance)\u001b[39m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m tolerance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    565\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcannot use ``method`` argument if any indexers are slice objects\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    566\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m indexer = \u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_sanitize_slice_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_sanitize_slice_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_sanitize_slice_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# unlike pandas, in xarray we never want to silently convert a\u001b[39;00m\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# slice indexer into an array indexer\u001b[39;00m\n\u001b[32m    575\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    576\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcannot represent labeled-based slice indexer for coordinate \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoord_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m with a slice over integer positions; the index is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33munsorted or non-unique\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    579\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/site-packages/pandas/core/indexes/datetimes.py:682\u001b[39m, in \u001b[36mDatetimeIndex.slice_indexer\u001b[39m\u001b[34m(self, start, end, step)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# GH#33146 if start and end are combinations of str and None and Index is not\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[38;5;66;03m# monotonic, we can not use Index.slice_indexer because it does not honor the\u001b[39;00m\n\u001b[32m    676\u001b[39m \u001b[38;5;66;03m# actual elements, is only searching for start and end\u001b[39;00m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    678\u001b[39m     check_str_or_none(start)\n\u001b[32m    679\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m check_str_or_none(end)\n\u001b[32m    680\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_monotonic_increasing\n\u001b[32m    681\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m mask = np.array(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    685\u001b[39m in_index = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6708\u001b[39m, in \u001b[36mIndex.slice_indexer\u001b[39m\u001b[34m(self, start, end, step)\u001b[39m\n\u001b[32m   6664\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mslice_indexer\u001b[39m(\n\u001b[32m   6665\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   6666\u001b[39m     start: Hashable | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   6667\u001b[39m     end: Hashable | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   6668\u001b[39m     step: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   6669\u001b[39m ) -> \u001b[38;5;28mslice\u001b[39m:\n\u001b[32m   6670\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6671\u001b[39m \u001b[33;03m    Compute the slice indexer for input labels and step.\u001b[39;00m\n\u001b[32m   6672\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6706\u001b[39m \u001b[33;03m    slice(1, 3, None)\u001b[39;00m\n\u001b[32m   6707\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6708\u001b[39m     start_slice, end_slice = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslice_locs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6710\u001b[39m     \u001b[38;5;66;03m# return a slice\u001b[39;00m\n\u001b[32m   6711\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(start_slice):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6934\u001b[39m, in \u001b[36mIndex.slice_locs\u001b[39m\u001b[34m(self, start, end, step)\u001b[39m\n\u001b[32m   6932\u001b[39m start_slice = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   6933\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m6934\u001b[39m     start_slice = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_slice_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   6935\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_slice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6936\u001b[39m     start_slice = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/alive-gpp-env/lib/python3.11/site-packages/pandas/core/indexes/base.py:6867\u001b[39m, in \u001b[36mIndex.get_slice_bound\u001b[39m\u001b[34m(self, label, side)\u001b[39m\n\u001b[32m   6865\u001b[39m     slc = lib.maybe_booleans_to_slice(slc.view(\u001b[33m\"\u001b[39m\u001b[33mu1\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   6866\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slc, np.ndarray):\n\u001b[32m-> \u001b[39m\u001b[32m6867\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   6868\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m slice bound for non-unique \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6869\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(original_label)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   6870\u001b[39m         )\n\u001b[32m   6872\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slc, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   6873\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m side == \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: \"Cannot get left slice bound for non-unique label: Timestamp('2024-01-02 00:01:17')\""
     ]
    }
   ],
   "source": [
    "repo = aclient.get_repo('wisc-alive/ALIVE_v3-icechunk-lightweight-model-test')\n",
    "session = repo.readonly_session(branch=\"main\")\n",
    "ds = xr.open_zarr(session.store, group='ET', consolidated=False)\n",
    "\n",
    "\n",
    "print(f\"Number of timestamps: {len(ds.t)}\")\n",
    "print(f\"Unique timestamps: {len(np.unique(ds.t))}\")\n",
    "duplicates = ds.t.to_series().duplicated(keep=False)\n",
    "print(ds.t.values[duplicates.values][:10])\n",
    "\n",
    "visualize_icechunk_data(repo, group=\"ET\", start_time=merged_datetime_range[0], end_time=merged_datetime_range[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alive-gpp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
