{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALIVE-ABI/alive/blob/main/GOES_ABI_LandSurfaceProduct_Downloads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B37I_MK6qqRN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import os # For Mac computers\n",
        "import sys\n",
        "from google.cloud import storage\n",
        "import datetime\n",
        "import pvlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x70x5SnfTW8G"
      },
      "source": [
        "# Inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80ZaLR4tRKQs"
      },
      "outputs": [],
      "source": [
        "amflxDirectory = \"Add directory of csv file containing Ameriflux site information (ie.lat, lon, elevation, etc.)\"\n",
        "year_list = [2022] #Add year(s) of interest\n",
        "bucket_name = 'gcp-public-data-goes-16' # Change to 'goes-17' or 'goes-18'\n",
        "output_directory = '/Users/**/Downloads' # Change output directory to local path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtFdnByKbnW5"
      },
      "outputs": [],
      "source": [
        "# Active Ameriflux sites (from \"https://ameriflux.lbl.gov/sites/site-search/\")\n",
        "ameriflux_US = pd.read_csv(amflxDirectory, index_col= 'Site Id')\n",
        "lats = ameriflux_US['Latitude']\n",
        "lons = ameriflux_US['Longitude']\n",
        "elevs = ameriflux_US['Elevation (m)']\n",
        "ameriflux_US = ameriflux_US.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ka6L0QeVQeM"
      },
      "outputs": [],
      "source": [
        "#List of all variables\n",
        "column_names = [\n",
        "       'local_time', 'utc_time','SZA','SAA','ZenAz','doy','hour',\n",
        "       'CMI_C01', 'DQF_C01', 'CMI_C02', 'DQF_C02', 'CMI_C03', 'DQF_C03',\n",
        "       'BRF1', 'BRF2', 'BRF3', 'BRF_DQF',\n",
        "       'LSA', 'LSA_DQF',\n",
        "       'ACM', 'ACM_DQF',\n",
        "       'AOD', 'AOD_DQF',\n",
        "       'ADP_aero', 'ADP_smk', 'ADP_dust', 'ADP_DQF',\n",
        "       'LST', 'LST_DQF',\n",
        "       'DSR', 'DSR_DQF', 'NDVI','NIRv','PAR','NIRvP'\n",
        "       ]\n",
        "\n",
        "# Make an empty dictionary\n",
        "amflx = {}\n",
        "# Iterate through the Ameriflux sites\n",
        "for index, row in ameriflux_US.iterrows():\n",
        "    # Add entries to the dictionary: Ameriflux site name, new dataframe\n",
        "    amflx[index] = pd.DataFrame(columns = column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTJPxyJsSRe8"
      },
      "source": [
        "# Ortho functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqNVTQnwSwZ_"
      },
      "outputs": [],
      "source": [
        "def LonLat2ABIangle(lon_deg, lat_deg, z, H, req, rpol, e, lon_0_deg):\n",
        "    '''This function finds the ABI elevation (y) and scanning (x) angles (radians) of point P,\n",
        "    given a latitude and longitude (degrees)'''\n",
        "\n",
        "    # Convert lat and lon from degrees to radians\n",
        "    lon = np.radians(lon_deg)\n",
        "    lat = np.radians(lat_deg)\n",
        "    lon_0 = np.radians(lon_0_deg)\n",
        "\n",
        "    # Geocentric latitude\n",
        "    lat_geo = np.arctan( (rpol**2 / req**2) * np.tan(lat) )\n",
        "\n",
        "    # Geocentric distance to point on the ellipsoid\n",
        "    _rc = rpol / np.sqrt(1 - (e**2)*(np.cos(lat_geo)**2)) # this is rc if point is on the ellipsoid\n",
        "    rc = _rc + z # this is rc if the point is offset from the ellipsoid by z (meters)\n",
        "\n",
        "    # Intermediate calculations\n",
        "    Sx = H - rc * np.cos(lat_geo) * np.cos(lon - lon_0)\n",
        "    Sy = -rc * np.cos(lat_geo) * np.sin(lon - lon_0)\n",
        "    Sz = rc * np.sin(lat_geo)\n",
        "\n",
        "    # Calculate x and y scan angles\n",
        "    y = np.arctan( Sz / Sx )\n",
        "    x = np.arcsin( -Sy / np.sqrt( Sx**2 + Sy**2 + Sz**2 ) )\n",
        "\n",
        "    return (x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFiSwMwrBlpO"
      },
      "outputs": [],
      "source": [
        "def ABIangle2LonLat(x, y, H, req, rpol, lon_0_deg):\n",
        "    '''This function finds the latitude and longitude (degrees) of point P\n",
        "    given x and y, the ABI elevation and scanning angle (radians)'''\n",
        "\n",
        "    # Intermediate calculations\n",
        "    a = np.sin(x)**2 + ( np.cos(x)**2 * ( np.cos(y)**2 + ( req**2 / rpol**2 ) * np.sin(y)**2 ) )\n",
        "    b = -2 * H * np.cos(x) * np.cos(y)\n",
        "    c = H**2 - req**2\n",
        "\n",
        "    rs = ( -b - np.sqrt( b**2 - 4*a*c ) ) / ( 2 * a ) # distance from satellite point (S) to P\n",
        "\n",
        "    Sx = rs * np.cos(x) * np.cos(y)\n",
        "    Sy = -rs * np.sin(x)\n",
        "    Sz = rs * np.cos(x) * np.sin(y)\n",
        "\n",
        "    # Calculate lat and lon\n",
        "    lat = np.arctan( ( req**2 / rpol**2 ) * ( Sz / np.sqrt( ( H - Sx )**2 + Sy**2 ) ) )\n",
        "    lat = np.degrees(lat) #*\n",
        "    lon = lon_0_deg - np.degrees( np.arctan( Sy / ( H - Sx )) )\n",
        "\n",
        "    return (lon,lat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNYf1NclGmuS"
      },
      "outputs": [],
      "source": [
        "def solar_position(lat, lon, elev):\n",
        "  '''This function calculates the solar geometry throughout the day at Ameriflux site from its lat, lon and elevation'''\n",
        "  date = timestamp.pd_asdatetime()\n",
        "  solar_pos = pvlib.solarposition.get_solarposition(time, lat, lon, elev)\n",
        "  SZA = solar_pos['zenith'].reset_index()\n",
        "  az = solar_pos['azimuth'].reset_index()\n",
        "  zenAz = az + SZA\n",
        "  df = pd.concat([df, zenith], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mv3z2qwBNwj"
      },
      "source": [
        "# Download Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqarXrf1SQoM"
      },
      "outputs": [],
      "source": [
        "def make_abi_timeseries(filename, product, unique_df):\n",
        "    '''Given a directory of GOES ABI products, the function creates a timeseries of data variables (specified in data_vars) for a single point (at lon, lat, elevation).\n",
        "\t   Returns a pandas dataframe and  outputs a csv file for each year (optionally, for each day).'''\n",
        "\n",
        "    # Make an empty dictionary for each site\n",
        "    unique_df = {}\n",
        "\n",
        "    # List the relevant variables for each product\n",
        "    product = product\n",
        "    if product == 'MCMIPF':\n",
        "      data_vars =     ['CMI_C01', 'DQF_C01', 'CMI_C02', 'DQF_C02', 'CMI_C03', 'DQF_C03']\n",
        "      column_names =  ['CMI_C01', 'DQF_C01', 'CMI_C02', 'DQF_C02', 'CMI_C03', 'DQF_C03']\n",
        "    if product == 'BRFF':\n",
        "      data_vars =     ['BRF1', 'BRF2', 'BRF3', 'DQF']\n",
        "      column_names =  ['BRF1', 'BRF2', 'BRF3', 'BRF_DQF']\n",
        "    if product == 'LSAF':\n",
        "      data_vars =     ['LSA', 'DQF']\n",
        "      column_names =  ['LSA', 'LSA_DQF']\n",
        "    if product == 'ACMF':\n",
        "      data_vars =     ['BCM', 'DQF']\n",
        "      column_names =  ['ACM', 'ACM_DQF']\n",
        "    if product == 'AODF':\n",
        "      data_vars =     ['AOD', 'DQF']\n",
        "      column_names =  ['AOD', 'AOD_DQF']\n",
        "    if product == 'ADPF':\n",
        "      data_vars =     ['Aerosol', 'Smoke', 'Dust','DQF']\n",
        "      column_names =  ['ADP_aero', 'ADP_smk', 'ADP_dust', 'ADP_DQF']\n",
        "    if product == 'LSTF':\n",
        "      data_vars =     ['LST', 'DQF']\n",
        "      column_names =  ['LST', 'LST_DQF']\n",
        "    if product == 'DSRF':\n",
        "      data_vars =     ['DSR', 'DQF']\n",
        "      column_names =  ['DSR', 'DSR_DQF']\n",
        "\n",
        "    print('New file:' + product)\n",
        "    f = xr.open_dataset(filename, decode_times=False)\\\n",
        "\n",
        "    # Access each Ameriflux Site dataframe by iterating through each dictionary item\n",
        "    for index, df in amflx.items():\n",
        "      site_df = pd.DataFrame(columns = [])\n",
        "\n",
        "      # For each Ameriflux site, set the lat, lon and elevation by referring to entries in the US Ameriflux Sites database\n",
        "      site_lat = ameriflux_US.loc[str(index), 'Latitude']\n",
        "      site_lon = ameriflux_US.loc[str(index), 'Longitude']\n",
        "      site_elev = ameriflux_US.loc[str(index), 'Elevation (m)']\n",
        "\n",
        "      # Read goes_imager_projection values needed for geometry calculations\n",
        "      # and compute the corresponding look angles (in radiance) for the lat, lon, elevation we are interested in\n",
        "      x_rad, y_rad = LonLat2ABIangle(site_lon,\n",
        "                                   site_lat,\n",
        "                                   site_elev,\n",
        "                                   f.goes_imager_projection.perspective_point_height + f.goes_imager_projection.semi_major_axis,\n",
        "                                   f.goes_imager_projection.semi_major_axis,\n",
        "                                   f.goes_imager_projection.semi_minor_axis,\n",
        "                                   0.0818191910435, # GRS-80 eccentricity\n",
        "                                   f.goes_imager_projection.longitude_of_projection_origin)\n",
        "\n",
        "      # Get the timestamp (UTC) for this observation\n",
        "      timestamp = pd.Timestamp(f.time_coverage_start).replace(tzinfo=None)\n",
        "\n",
        "      # Create an empty dictionary we will populate with values from file f\n",
        "      this_row_dict = {}\n",
        "\n",
        "      # Create an empty list of the same length as data_vars to hold each variable's value\n",
        "      # and create a matching empty list to hold each column's name in the matching\n",
        "      values = ['' for v in data_vars]\n",
        "      column = ['' for v in data_vars]\n",
        "\n",
        "      # For each variable we are interested, specified in the list \"data_vars\"\n",
        "      # Create an empty dataframe\n",
        "      one_site =  pd.DataFrame(columns=[])\n",
        "\n",
        "      for i, var in enumerate(data_vars):\n",
        "        # Find corresponding pixel data_var value nearest to these scan angles y_rad and x_rad\n",
        "        values[i] = f[var].sel(y=y_rad, x=x_rad, method='nearest').values.mean()\n",
        "        column[i] = column_names[i]\n",
        "\n",
        "        # Create a dictionary for this row of values (where each row is a GOES-R observation time)\n",
        "        this_row_dict = dict(zip(column, values ))\n",
        "        this_row_dict['timestamp'] = timestamp\n",
        "\n",
        "        # Concatenate the half-hourly measurements for each hour\n",
        "        one_site = pd.concat([one_site, pd.DataFrame([this_row_dict])], ignore_index=True)\n",
        "\n",
        "      # Drop duplicates if there are any, keep the last one\n",
        "      one_site = one_site.drop_duplicates(['timestamp'], keep='last', inplace=False)\n",
        "\n",
        "      # Set the dataframe intext to the timestamp column\n",
        "      one_site = one_site.set_index('timestamp', inplace = False, verify_integrity = False)\n",
        "\n",
        "      # Store two half-hourly observations at the site in a dictionary\n",
        "      unique_df[str(index)] = one_site\n",
        "\n",
        "    return unique_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUroHCrbdqvt"
      },
      "outputs": [],
      "source": [
        "def DSR_make_abi_timeseries(filename, product, unique_df):\n",
        "    '''Given a directory of GOES ABI products, create a timeseries of data variables (specified in data_vars) for a single point (at lon, lat, elevation).\n",
        "\t   Returns a pandas dataframe, optional output to a csv file.'''\n",
        "\n",
        "    unique_df = {}\n",
        "\n",
        "    #List the relevant variables for each product\n",
        "    product = product\n",
        "    if product == 'DSRF':\n",
        "      data_vars =     ['DSR', 'DQF']\n",
        "      column_names =  ['DSR', 'DSR_DQF']\n",
        "\n",
        "    #print('Creating a timeseries of each of these variables {variable} from {product}'.format(variable=data_vars, product = product))\n",
        "    print('New file:' + product)\n",
        "    f = xr.open_dataset(filename)\n",
        "\n",
        "    #Access each Ameriflux Site dataframe by iterating through each dictionary item\n",
        "    for index, df in amflx.items():\n",
        "      #For each Ameriflux site, set the lat, lon and elevation by referring to entries in the US Ameriflux Sites database\n",
        "      site_lat = ameriflux_US.loc[str(index), 'Latitude']\n",
        "      site_lon = ameriflux_US.loc[str(index), 'Longitude']\n",
        "      site_elev = ameriflux_US.loc[str(index), 'Elevation (m)']\n",
        "\n",
        "      # Read goes_lat_lon_projection values needed for geometry calculations\n",
        "      # and compute the corresponding look angles (in radiance) for the lat, lon, elevation we are interested in\n",
        "      x_rad, y_rad = LonLat2ABIangle(site_lon,\n",
        "                                     site_lat,\n",
        "                                     site_elev,\n",
        "                                     35786023.0 + f.goes_lat_lon_projection.semi_major_axis, # DSR missing metadata, setting perspective point height = 35786023.0\n",
        "                                     f.goes_lat_lon_projection.semi_major_axis,\n",
        "                                     f.goes_lat_lon_projection.semi_minor_axis,\n",
        "                                     0.0818191910435, # GRS-80 eccentricity\n",
        "                                     -75.0) # DSR missing longitude of projection origin, set to -75.0\n",
        "\n",
        "      lat, lon = ABIangle2LonLat(x_rad,\n",
        "                                 y_rad,\n",
        "                                 35786023.0 + f.goes_lat_lon_projection.semi_major_axis,\n",
        "                                 f.goes_lat_lon_projection.semi_major_axis,\n",
        "                                 f.goes_lat_lon_projection.semi_minor_axis,\n",
        "                                 -75.0)\n",
        "\n",
        "      # Get the timestamp for this observation (UTC)\n",
        "      timestamp = pd.Timestamp(f.time_coverage_start).replace(tzinfo=None)\n",
        "\n",
        "      # Create an empty dictionary we will populate with values from file f\n",
        "      this_row_dict = {}\n",
        "\n",
        "      # Create an empty list of the same length as data_vars to hold each variable's value\n",
        "      # and create a matching empty list to hold each column's name in the matching\n",
        "      values = ['' for v in data_vars]\n",
        "      column = ['' for v in data_vars]\n",
        "\n",
        "      # For each variable we are interested, specified in the list \"data_vars\"\n",
        "      one_site =  pd.DataFrame(columns=[])\n",
        "\n",
        "      for i, var in enumerate(data_vars):\n",
        "        # Find corresponding pixel data_var value nearest to the lat/lon\n",
        "        values[i] = f[var].sel(lat= lat, lon= lon, method='nearest').values.mean()\n",
        "        column[i] = column_names[i]\n",
        "\n",
        "        # Create a dictionary for this row of values (where each row is a GOES-R observation time)\n",
        "        this_row_dict = dict(zip(column, values ))\n",
        "        this_row_dict['timestamp'] = timestamp\n",
        "\n",
        "        one_site = pd.concat([one_site, pd.DataFrame([this_row_dict])], ignore_index=True)\n",
        "\n",
        "      # Drop duplicates if there are any, keep the first one\n",
        "      one_site = one_site.drop_duplicates(['timestamp'], keep='last', inplace=False)\n",
        "\n",
        "      # Set the dataframe intext to the timestamp column\n",
        "      one_site = one_site.set_index('timestamp', inplace = False, verify_integrity = False)\n",
        "\n",
        "      # Store one hourly observation at the site in a dictionary\n",
        "      unique_df[str(index)] = one_site\n",
        "\n",
        "    return unique_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHCGwxesytEL"
      },
      "outputs": [],
      "source": [
        "def download_blobs(bucket_name, file_prefix, directory, product, unique_df):\n",
        "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
        "\n",
        "    # Create anonymous Google Cloud storage credentials to access public buckets\n",
        "    storage_client = storage.Client.create_anonymous_client()\n",
        "\n",
        "    # Access GOES-R bucket for specific date/time/product and list its content\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "    blobs = storage_client.list_blobs(bucket_name, prefix = file_prefix)\n",
        "    blob_list = list(blobs)\n",
        "\n",
        "    # For land surface temperature (LST)\n",
        "    if product == 'LSTF':\n",
        "        one_blob = blob_list[0]\n",
        "        location, blob_filename= os.path.split(one_blob.name)\n",
        "        destination_file_name = directory + blob_filename\n",
        "        eachBlob = storage.Blob(one_blob.name, bucket)\n",
        "        # Download LST file to local destination\n",
        "        file = eachBlob.download_to_filename(destination_file_name)\n",
        "        # Make and store single hourly LST observation\n",
        "        one_df = make_abi_timeseries(destination_file_name, product, unique_df)\n",
        "        os.remove(destination_file_name)\n",
        "\n",
        "    # For downward shortwave radiation (DSR)\n",
        "    elif product == 'DSRF':\n",
        "        one_blob = blob_list[0]\n",
        "        location, blob_filename= os.path.split(one_blob.name)\n",
        "        destination_file_name = directory + blob_filename\n",
        "        eachBlob = storage.Blob(one_blob.name, bucket)\n",
        "        # Download DSR file to local destination\n",
        "        file = eachBlob.download_to_filename(destination_file_name)\n",
        "        # Make and store single hourly DSR observation\n",
        "        one_df = DSR_make_abi_timeseries(destination_file_name, product, unique_df)\n",
        "        os.remove(destination_file_name)\n",
        "\n",
        "    # For all other ABI L2 products (with refresh rates > once/hour)\n",
        "    else:\n",
        "        blob_dict = {}\n",
        "        one_df = {}\n",
        "        # Get files 0 and 3 from the list (around 10 and 30 mins past the hour)\n",
        "        halfHour_blobs = [blob_list[0],blob_list[3]]\n",
        "        # Loop through both files\n",
        "        for blob in halfHour_blobs:\n",
        "            location, blob_filename= os.path.split(blob.name)\n",
        "            destination_file_name = directory + blob_filename\n",
        "            eachBlob = storage.Blob(blob.name, bucket)\n",
        "            # Download each file to local destination\n",
        "            file = eachBlob.download_to_filename(destination_file_name)\n",
        "            # Make and store single half-hourly observation\n",
        "            data_table = make_abi_timeseries(destination_file_name, product, unique_df)\n",
        "            blob_dict[blob] = data_table\n",
        "            os.remove(destination_file_name)\n",
        "        # Combine two half hourly observations into one dataframe\n",
        "        for index, row in ameriflux_US.iterrows():\n",
        "            one_table = pd.concat([blob_dict[blob_list[0]][index],\n",
        "                                   blob_dict[blob_list[3]][index]],\n",
        "                                )\n",
        "            one_df[index] = one_table\n",
        "    return one_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve35WE8TOtXf"
      },
      "outputs": [],
      "source": [
        "def concat_days(index,day_dict) :\n",
        "    \"This function concatenates each daily dataframe into one yearly dataframe. Add day 366 for Leap Years.\"\n",
        "    one_year = pd.concat([\n",
        "    day_dict['001'][index], day_dict['002'][index], day_dict['003'][index], day_dict['004'][index], day_dict['005'][index],\n",
        "    day_dict['006'][index], day_dict['007'][index], day_dict['008'][index], day_dict['009'][index], day_dict['010'][index],\n",
        "    day_dict['011'][index], day_dict['012'][index], day_dict['013'][index], day_dict['014'][index], day_dict['015'][index],\n",
        "    day_dict['016'][index], day_dict['017'][index], day_dict['018'][index], day_dict['019'][index], day_dict['020'][index],\n",
        "    day_dict['021'][index], day_dict['022'][index], day_dict['023'][index], day_dict['024'][index], day_dict['025'][index],\n",
        "    day_dict['026'][index], day_dict['027'][index], day_dict['028'][index], day_dict['029'][index], day_dict['030'][index],\n",
        "    day_dict['031'][index], day_dict['032'][index], day_dict['033'][index], day_dict['034'][index], day_dict['035'][index],\n",
        "    day_dict['036'][index], day_dict['037'][index], day_dict['038'][index], day_dict['039'][index], day_dict['040'][index],\n",
        "    day_dict['041'][index], day_dict['042'][index], day_dict['043'][index], day_dict['044'][index], day_dict['045'][index],\n",
        "    day_dict['046'][index], day_dict['047'][index], day_dict['048'][index], day_dict['049'][index], day_dict['050'][index],\n",
        "    day_dict['051'][index], day_dict['052'][index], day_dict['053'][index], day_dict['054'][index], day_dict['055'][index],\n",
        "    day_dict['056'][index], day_dict['057'][index], day_dict['058'][index], day_dict['059'][index], day_dict['060'][index],\n",
        "    day_dict['061'][index], day_dict['062'][index], day_dict['063'][index], day_dict['064'][index], day_dict['065'][index],\n",
        "    day_dict['066'][index], day_dict['067'][index], day_dict['068'][index], day_dict['069'][index], day_dict['070'][index],\n",
        "    day_dict['071'][index], day_dict['072'][index], day_dict['073'][index], day_dict['074'][index], day_dict['075'][index],\n",
        "    day_dict['076'][index], day_dict['077'][index], day_dict['078'][index], day_dict['079'][index], day_dict['080'][index],\n",
        "    day_dict['081'][index], day_dict['082'][index], day_dict['083'][index], day_dict['084'][index], day_dict['085'][index],\n",
        "    day_dict['086'][index], day_dict['087'][index], day_dict['088'][index], day_dict['089'][index], day_dict['090'][index],\n",
        "    day_dict['091'][index], day_dict['092'][index], day_dict['093'][index], day_dict['094'][index], day_dict['095'][index],\n",
        "    day_dict['096'][index], day_dict['097'][index], day_dict['098'][index], day_dict['099'][index], day_dict['100'][index],\n",
        "    day_dict['101'][index], day_dict['102'][index], day_dict['103'][index], day_dict['104'][index], day_dict['105'][index],\n",
        "    day_dict['106'][index], day_dict['107'][index], day_dict['108'][index], day_dict['109'][index], day_dict['110'][index],\n",
        "    day_dict['111'][index], day_dict['112'][index], day_dict['113'][index], day_dict['114'][index], day_dict['115'][index],\n",
        "    day_dict['116'][index], day_dict['117'][index], day_dict['118'][index], day_dict['119'][index], day_dict['120'][index],\n",
        "    day_dict['121'][index], day_dict['122'][index], day_dict['123'][index], day_dict['124'][index], day_dict['125'][index],\n",
        "    day_dict['126'][index], day_dict['127'][index], day_dict['128'][index], day_dict['129'][index], day_dict['130'][index],\n",
        "    day_dict['131'][index], day_dict['132'][index], day_dict['133'][index], day_dict['134'][index], day_dict['135'][index],\n",
        "    day_dict['136'][index], day_dict['137'][index], day_dict['138'][index], day_dict['139'][index], day_dict['140'][index],\n",
        "    day_dict['141'][index], day_dict['142'][index], day_dict['143'][index], day_dict['144'][index], day_dict['145'][index],\n",
        "    day_dict['146'][index], day_dict['147'][index], day_dict['148'][index], day_dict['149'][index], day_dict['150'][index],\n",
        "    day_dict['151'][index], day_dict['152'][index], day_dict['153'][index], day_dict['154'][index], day_dict['155'][index],\n",
        "    day_dict['156'][index], day_dict['157'][index], day_dict['158'][index], day_dict['159'][index], day_dict['160'][index],\n",
        "    day_dict['161'][index], day_dict['162'][index], day_dict['163'][index], day_dict['164'][index], day_dict['165'][index],\n",
        "    day_dict['166'][index], day_dict['167'][index], day_dict['168'][index], day_dict['169'][index], day_dict['170'][index],\n",
        "    day_dict['171'][index], day_dict['172'][index], day_dict['173'][index], day_dict['174'][index], day_dict['175'][index],\n",
        "    day_dict['176'][index], day_dict['177'][index], day_dict['178'][index], day_dict['179'][index], day_dict['180'][index],\n",
        "    day_dict['181'][index], day_dict['182'][index], day_dict['183'][index], day_dict['184'][index], day_dict['185'][index],\n",
        "    day_dict['186'][index], day_dict['187'][index], day_dict['188'][index], day_dict['189'][index], day_dict['190'][index],\n",
        "    day_dict['191'][index], day_dict['192'][index], day_dict['193'][index], day_dict['194'][index], day_dict['195'][index],\n",
        "    day_dict['196'][index], day_dict['197'][index], day_dict['198'][index], day_dict['199'][index], day_dict['200'][index],\n",
        "    day_dict['201'][index], day_dict['202'][index], day_dict['203'][index], day_dict['204'][index], day_dict['205'][index],\n",
        "    day_dict['206'][index], day_dict['207'][index], day_dict['208'][index], day_dict['209'][index], day_dict['210'][index],\n",
        "    day_dict['211'][index], day_dict['212'][index], day_dict['213'][index], day_dict['214'][index], day_dict['215'][index],\n",
        "    day_dict['216'][index], day_dict['217'][index], day_dict['218'][index], day_dict['219'][index], day_dict['220'][index],\n",
        "    day_dict['221'][index], day_dict['222'][index], day_dict['223'][index], day_dict['224'][index], day_dict['225'][index],\n",
        "    day_dict['226'][index], day_dict['227'][index], day_dict['228'][index], day_dict['229'][index], day_dict['230'][index],\n",
        "    day_dict['231'][index], day_dict['232'][index], day_dict['233'][index], day_dict['234'][index], day_dict['235'][index],\n",
        "    day_dict['236'][index], day_dict['237'][index], day_dict['238'][index], day_dict['239'][index], day_dict['240'][index],\n",
        "    day_dict['241'][index], day_dict['242'][index], day_dict['243'][index], day_dict['244'][index], day_dict['245'][index],\n",
        "    day_dict['246'][index], day_dict['247'][index], day_dict['248'][index], day_dict['249'][index], day_dict['250'][index],\n",
        "    day_dict['251'][index], day_dict['252'][index], day_dict['253'][index], day_dict['254'][index], day_dict['255'][index],\n",
        "    day_dict['256'][index], day_dict['257'][index], day_dict['258'][index], day_dict['259'][index], day_dict['260'][index],\n",
        "    day_dict['261'][index], day_dict['262'][index], day_dict['263'][index], day_dict['264'][index], day_dict['265'][index],\n",
        "    day_dict['266'][index], day_dict['267'][index], day_dict['268'][index], day_dict['269'][index], day_dict['270'][index],\n",
        "    day_dict['271'][index], day_dict['272'][index], day_dict['273'][index], day_dict['274'][index], day_dict['275'][index],\n",
        "    day_dict['276'][index], day_dict['277'][index], day_dict['278'][index], day_dict['279'][index], day_dict['280'][index],\n",
        "    day_dict['281'][index], day_dict['282'][index], day_dict['283'][index], day_dict['284'][index], day_dict['285'][index],\n",
        "    day_dict['286'][index], day_dict['287'][index], day_dict['288'][index], day_dict['289'][index], day_dict['290'][index],\n",
        "    day_dict['291'][index], day_dict['292'][index], day_dict['293'][index], day_dict['294'][index], day_dict['295'][index],\n",
        "    day_dict['296'][index], day_dict['297'][index], day_dict['298'][index], day_dict['299'][index], day_dict['300'][index],\n",
        "    day_dict['301'][index], day_dict['302'][index], day_dict['303'][index], day_dict['304'][index], day_dict['305'][index],\n",
        "    day_dict['306'][index], day_dict['307'][index], day_dict['308'][index], day_dict['309'][index], day_dict['310'][index],\n",
        "    day_dict['311'][index], day_dict['312'][index], day_dict['313'][index], day_dict['314'][index], day_dict['315'][index],\n",
        "    day_dict['316'][index], day_dict['317'][index], day_dict['318'][index], day_dict['319'][index], day_dict['320'][index],\n",
        "    day_dict['321'][index], day_dict['322'][index], day_dict['323'][index], day_dict['324'][index], day_dict['325'][index],\n",
        "    day_dict['326'][index], day_dict['327'][index], day_dict['328'][index], day_dict['329'][index], day_dict['330'][index],\n",
        "    day_dict['331'][index], day_dict['332'][index], day_dict['333'][index], day_dict['334'][index], day_dict['335'][index],\n",
        "    day_dict['336'][index], day_dict['337'][index], day_dict['338'][index], day_dict['339'][index], day_dict['340'][index],\n",
        "    day_dict['341'][index], day_dict['342'][index], day_dict['343'][index], day_dict['344'][index], day_dict['345'][index],\n",
        "    day_dict['346'][index], day_dict['347'][index], day_dict['348'][index], day_dict['349'][index], day_dict['350'][index],\n",
        "    day_dict['351'][index], day_dict['352'][index], day_dict['353'][index], day_dict['354'][index], day_dict['355'][index],\n",
        "    day_dict['356'][index], day_dict['357'][index], day_dict['358'][index], day_dict['359'][index], day_dict['360'][index],\n",
        "    day_dict['361'][index], day_dict['362'][index], day_dict['363'][index], day_dict['364'][index], day_dict['365'][index],\n",
        "    ])\n",
        "    return one_year\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hCwYBTScy6-"
      },
      "source": [
        "# Run functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElnXKgHHOtXg"
      },
      "outputs": [],
      "source": [
        "# Create empty dictionaries\n",
        "variable_dict = {}\n",
        "unique_df = {}\n",
        "day_dict = {}\n",
        "each_day_dict = {}\n",
        "hour_dict = {}\n",
        "\n",
        "# Loop through each year\n",
        "for year in year_list: #Year or list of years\n",
        "  year = year\n",
        "\n",
        "  # Loop through each day\n",
        "  for day in range(1,366): #Change for leap year\n",
        "    day = day\n",
        "    day = str(day).zfill(3)\n",
        "    print(\"NEXT DAY! Day = \" + str(day))\n",
        "\n",
        "    # Loop through each hour\n",
        "    for hour in range(0,24):\n",
        "        hour = hour\n",
        "        hour = str(hour).zfill(2)\n",
        "        print(\"Next hour! Hour = \" + str(hour))\n",
        "        file_list = []\n",
        "\n",
        "        # Loop through each product\n",
        "        for product in ['MCMIPF', 'BRFF', 'LSAF', 'ACMF', 'AODF', 'ADPF', 'LSTF', 'DSRF']:\n",
        "            product = product\n",
        "            folder_path = 'ABI-L2-{product}/{year}/{day}/{hour}/'.format(product = product,\n",
        "                                                                     year = year,\n",
        "                                                                     day = day,\n",
        "                                                                     hour = hour)\n",
        "            short_folder_path = 'ABI-L2-{product}/{year}/{day}/'.format(product = product,\n",
        "                                                                     year = year,\n",
        "                                                                     day = day)\n",
        "            local_folder = local_path + short_folder_path\n",
        "            os.makedirs(local_folder, exist_ok=True)\n",
        "\n",
        "            # Download GOES-R ABI product file to local directory for temporary storage\n",
        "            each_var = download_blobs(bucket_name, folder_path, local_folder, product, unique_df)\n",
        "\n",
        "            #Store each dataframe of each product observation in dictionary\n",
        "            variable_dict[str(product)] = each_var\n",
        "\n",
        "        #Iterate through the Ameriflux sites\n",
        "        site_dict = {}\n",
        "        for index, row in ameriflux_US.iterrows():\n",
        "\n",
        "            # Concatenate observations from each ABI product into a single dataframe with half-hourly timesteps\n",
        "            all_var = pd.concat([variable_dict['MCMIPF'][index],\n",
        "                                 variable_dict['BRFF'][index],\n",
        "                                 variable_dict['LSAF'][index],\n",
        "                                 variable_dict['ACMF'][index],\n",
        "                                 variable_dict['AODF'][index],\n",
        "                                 variable_dict['ADPF'][index],\n",
        "                                 variable_dict['LSTF'][index],\n",
        "                                 variable_dict['DSRF'][index],\n",
        "                                 ], axis=1, join='outer')\n",
        "            # Calculate indices 'NDVI' and 'NIRv' from surface reflectances\n",
        "            BRF1 = all_var['BRF1']\n",
        "            BRF2 = all_var['BRF2']\n",
        "            BRF3 = all_var['BRF3']\n",
        "            all_var['NDVI'] = (BRF3 - BRF2)/(BRF3 + BRF2)\n",
        "            all_var['NIRv'] = BRF3 * all_var['NDVI']\n",
        "\n",
        "            # Estimate PAR and NIRv from DSR\n",
        "            all_var['PAR'] = 0.45 * all_var['DSR'] # PAR in W/m2\n",
        "            all_var['NIRvP'] = all_var['NIRv'] * all_var['PAR']\n",
        "\n",
        "            #Manage UTC timestamps, add local time, and add other datetime variables\n",
        "            all_var = all_var.reset_index()\n",
        "            all_var['UTC_TIME'] = pd.to_datetime(all_var.timestamp)\n",
        "            all_var = all_var.set_index('timestamp')\n",
        "            hr_offset = float(ameriflux_US.loc[str(index), 'Timezone/UTC offset'].split('+')[1])\n",
        "            all_var['LOCAL_TIME'] = all_var['utc_time'] - datetime.timedelta(hours=hr_offset)\n",
        "            all_var['DOY'] = all_var.local_time.dt.dayofyear\n",
        "            all_var['HOUR'] = all_var.local_time.dt.hour\n",
        "\n",
        "            # Calculate solar position using pvlib module\n",
        "            solar_pos = pvlib.solarposition.get_solarposition(all_var['utc_time'],\n",
        "                                                              ameriflux_US.loc[str(index), 'Latitude'],\n",
        "                                                              ameriflux_US.loc[str(index), 'Longitude'],\n",
        "                                                              ameriflux_US.loc[str(index), 'Elevation (m)'])\n",
        "            all_var['SZA'] = solar_pos['zenith']\n",
        "            all_var['SAA'] = solar_pos['azimuth']\n",
        "            # Add solar zenith angle and solar azimuth angle to get unique solar position\n",
        "            all_var['SOLAR_POS'] = all_var['SZA'] + all_var['SAA']\n",
        "\n",
        "            # Add hourly dataframe for all products at site to a site dictionary\n",
        "            site_dict[index] = all_var\n",
        "\n",
        "        # Add hourly site dictionaries to an hourly dictionary\n",
        "        hour_dict[str(hour)] = site_dict\n",
        "\n",
        "    each_day_dict = {}\n",
        "    # Iterate through each hour of the day\n",
        "    for index, row in ameriflux_US.iterrows():\n",
        "      # Concatenate dataframes from each hour of the day at the site (stored in hourly dictionary)\n",
        "      one_day = pd.concat(  [hour_dict['00'][index], hour_dict['01'][index], hour_dict['02'][index],\n",
        "                             hour_dict['03'][index], hour_dict['04'][index], hour_dict['05'][index],\n",
        "                             hour_dict['06'][index], hour_dict['07'][index], hour_dict['08'][index],\n",
        "                             hour_dict['09'][index], hour_dict['10'][index], hour_dict['11'][index],\n",
        "                             hour_dict['12'][index], hour_dict['13'][index], hour_dict['14'][index],\n",
        "                             hour_dict['15'][index], hour_dict['16'][index], hour_dict['17'][index],\n",
        "                             hour_dict['18'][index], hour_dict['19'][index], hour_dict['20'][index],\n",
        "                             hour_dict['21'][index], hour_dict['22'][index], hour_dict['23'][index],\n",
        "                             ])\n",
        "      # Interpolate half-hour observations (using cubic spline interpolation) between hourly DSR and LST observations\n",
        "      if np.sum(one_day['LST'].count()) > 1:\n",
        "        one_day['LST'] = one_day['LST'].interpolate(method='cubicspline', limit = 1)\n",
        "        one_day['LST_DQF'] = one_day['LST_DQF'].fillna(method=\"ffill\")\n",
        "      if np.sum(one_day['DSR'].count()) > 1:\n",
        "        one_day['DSR'] = one_day['DSR'].interpolate(method='cubicspline', limit = 1)\n",
        "        one_day['DSR_DQF'] = one_day['DSR_DQF'].fillna(method=\"ffill\")\n",
        "\n",
        "      # Uncomment below to download files daily and write-over previous days\n",
        "      #csv_file = one_day.to_csv('/<<local path>>/'+ str(index) + '.csv', columns = column_names)\n",
        "\n",
        "      # Store each day dataframe for site in a daily dictionary\n",
        "      each_day_dict[index] = one_day\n",
        "\n",
        "    # Store each daily datafram eover the year in day dictionary\n",
        "    day_dict[day] = each_day_dict\n",
        "    print(\"Day complete\")\n",
        "\n",
        "  each_year_dict = {}\n",
        "\n",
        "  # Iterate through each Ameriflux site\n",
        "  for index, row in ameriflux_US.iterrows():\n",
        "\n",
        "      # Concatenate dataframes from each day of the year at the site (stored in day dictionary)\n",
        "      one_year = concat_days(index, day_dict)\n",
        "\n",
        "      # Convert pandas dataframe to csv file for each year and sotre at local directory\n",
        "      csv_file = one_year.to_csv(local_path + '/amflx_data/byYear/'+ str(index) + '.csv', columns = column_names)\n",
        "      each_year_dict[index] = one_year\n",
        "  print(\"Year complete\")\n",
        "\n",
        "  #For multiple years, concatenate dataframe from each_year_dict"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python [conda env:download]",
      "language": "python",
      "name": "conda-env-download-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
